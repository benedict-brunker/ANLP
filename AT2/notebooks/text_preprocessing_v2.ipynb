{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3YqOhjWKCVcn"
      },
      "source": [
        "My first attempt at preprocessing resulted in screenplays that were still much too large for neural networks.  Here I'll try to aggressively cut down the size of the screenplays as much as possible, while hopefully still retaining relevant info."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UUQd8hokmCVj"
      },
      "source": [
        "# 0. Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 521
        },
        "id": "w9JsuUpE-Spa",
        "outputId": "cb5dd9f3-0e80-4e15-c53d-720b2d3c3ed7"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "root_path = r'C:\\\\Users\\bened\\DataScience\\ANLP\\AT2\\\\36118_NLP_Spring\\\\CSVs'\n",
        "\n",
        "df_aus = pd.read_csv(f'{root_path}\\\\df_aus.csv', index_col=0)\n",
        "df_aus.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfIUP540TCIy",
        "outputId": "b87c02e8-7fa2-4765-a2a1-eebeb0311955"
      },
      "outputs": [],
      "source": [
        "df_aus.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "roxbury = df_aus['screenplay'][0][:1000]\n",
        "roxbury"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(roxbury)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(df_aus['screenplay'][0][1000:2000]) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76xhJyLYEkV2"
      },
      "source": [
        "# 1. Text Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_PMqCU_V2tW"
      },
      "source": [
        "## Sentence Tokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nmXnBDloV4uE"
      },
      "source": [
        "Before taking any additional steps, we're going to try to train a sentence tokenizer on the whole corpus, for later use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "! pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pom3ZsyRV9Td"
      },
      "outputs": [],
      "source": [
        "# build corpus\n",
        "\n",
        "corpus = (\"\\n\"*10).join(df_aus['screenplay'].values)\n",
        "corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f28WN-KFXJ9O",
        "outputId": "dc02b4be-a292-42b5-a0b5-5f15c0f45d1d"
      },
      "outputs": [],
      "source": [
        "print(corpus[:1000])\n",
        "print(\"-\"*50)\n",
        "print(corpus[(len(corpus)-1000):len(corpus)])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chj1cKqPWu9s"
      },
      "outputs": [],
      "source": [
        "screenplay_tokenizer = PunktSentenceTokenizer(train_text=corpus)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sra7gBj4YF7P"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "with open('sentence_tokenizer.pkl', 'wb') as f:\n",
        "  pickle.dump(screenplay_tokenizer, f)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kxs0x3wEoHe"
      },
      "source": [
        "Let's take a look at the first screenplay in the data, 'A Night at the Roxbury', to get a sense of how to approach this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 218
        },
        "collapsed": true,
        "id": "cCKPu4-iCRyr",
        "outputId": "b22ec5b3-c8ef-4595-d2c9-2ca7c81ee1b8"
      },
      "outputs": [],
      "source": [
        "roxbury_raw = df_aus.at[0, 'screenplay']\n",
        "roxbury_raw[:1000]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kd4-vtBdE60Q"
      },
      "source": [
        "- What's the first sequence here that might actually be relevant?  I would say perhaps the lines \"night falls and partytime begins.\n",
        "- Formatting:  The first several lines are basically metadata.  This is signified by subsequent strings of \\n\\t.  \n",
        "- The string \"FADE IN\" or \"EXT.\" is roughly where the screenplay proper begins.\n",
        "\n",
        "Let's look at a random sampling to determine a pattern."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MgFfcLAFEy0G",
        "outputId": "15b316b8-36da-4b16-ed01-99b88db6679a"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "screenplay_texts = list(df_aus['screenplay'].values)\n",
        "\n",
        "screenplay_sample = random.sample(screenplay_texts, 10)\n",
        "\n",
        "# print first 1000 chars of each screenplay\n",
        "\n",
        "for s in screenplay_sample:\n",
        "  print(s[:1000])\n",
        "  print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkXVZwGDG7Ns"
      },
      "source": [
        "- For actual screenplays, the action seems to begin with \"EXT\" or \"INT\".  Some of the data here are scripts, which don't follow this format.  So ideally, we truncate everything before EXT|INT, unless there's no match for either, in which case we truncate nothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjqMkct5HIUP"
      },
      "source": [
        "## truncate opening metadata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Nx6O93OuGPHa",
        "outputId": "80b4706d-07b3-4294-b9c4-6bba3d0c6710"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "pat = re.compile(r'EXT|INT')\n",
        "\n",
        "def truncate_metadata(screenplay):\n",
        "  match = re.search(pat, screenplay)\n",
        "  if match:\n",
        "    cutoff = match.end() + 1\n",
        "    # return the string from start\n",
        "    return screenplay[cutoff:]\n",
        "  # else return the whole screenplay\n",
        "  else:\n",
        "    return screenplay\n",
        "\n",
        "# beta test\n",
        "roxbury_truncated = truncate_metadata(roxbury_raw)\n",
        "print(roxbury_truncated[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prxSwPO4KfOl"
      },
      "source": [
        "We'll also keep updating stopwords bank as we go along."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qkGbeihmLZup",
        "outputId": "764feb0e-6d91-424f-c8b1-d58edf710cb6"
      },
      "outputs": [],
      "source": [
        "print(stop_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5Kojb-tKjC9",
        "outputId": "76aa2cc2-95a5-47f6-938f-4f9a7b803802"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "5I58NNiJI-JY",
        "outputId": "41c67a73-2e61-43e3-bc9a-d22008787e52"
      },
      "outputs": [],
      "source": [
        "# beta test for a script\n",
        "df_aus[df_aus['title'] == 'The Nightmare Before Christmas']\n",
        "nightmare = df_aus.at[787, 'screenplay']\n",
        "nightmare_truncated = truncate_metadata(nightmare)\n",
        "print(nightmare_truncated[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 458
        },
        "collapsed": true,
        "id": "WBI3OPECJsMx",
        "outputId": "2f1941e0-3f86-49aa-d0ab-d01bd097c677"
      },
      "outputs": [],
      "source": [
        "# seems to be working okay, so let's apply to all screenplays\n",
        "truncated_screenplays = df_aus['screenplay'].apply(truncate_metadata)\n",
        "truncated_screenplays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBzpBMviLntT"
      },
      "source": [
        "let's now see if there are any patterns to the end of screenplays"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bF2bRkOOKNjh",
        "outputId": "dac98dea-7ca2-422f-8e0d-f8e517b47a9b"
      },
      "outputs": [],
      "source": [
        "screenplay_sample = random.sample(screenplay_texts, 10)\n",
        "\n",
        "# print first 1000 chars of each screenplay\n",
        "\n",
        "for s in screenplay_sample:\n",
        "  print(s[(len(s)-1000):len(s)])\n",
        "  print(\"-\"*50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3_2qFB2MU1I"
      },
      "source": [
        "we wouldn't really be removing enough words here for it to be worthwhile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7LAnD5I6May4"
      },
      "source": [
        "## remove allcaps"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRbWe9kQMedJ"
      },
      "source": [
        "In general, it seems that words in all capital letters are either character names or provide photographic direction.  Neither are really releavnt to us. We will first tokenize into words, and then remove words that are all-caps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tYKHcFsiL_5F",
        "outputId": "078239f8-392f-4b89-8f23-994eb1aec305"
      },
      "outputs": [],
      "source": [
        "# first I want to see how nltk.word_tokenize will behave\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "roxbury_tokens = word_tokenize(roxbury_truncated, preserve_line=True)\n",
        "print(roxbury_tokens[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2s0L-dKnO4Bm",
        "outputId": "e66c0365-eee4-43eb-e452-d3691c728484"
      },
      "outputs": [],
      "source": [
        "# filter out capital letters\n",
        "def filter_allcaps(tokens):\n",
        "  filtered_tokens = []\n",
        "  for t in tokens:\n",
        "    if t.isupper() == False:\n",
        "      filtered_tokens.append(t)\n",
        "    else:\n",
        "      continue\n",
        "  return filtered_tokens\n",
        "\n",
        "roxbury_lower = filter_allcaps(roxbury_tokens)\n",
        "print(roxbury_lower[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k0YmVR5GP_i9",
        "outputId": "1ec5af12-3c90-4504-f84b-6e60b59515d6"
      },
      "outputs": [],
      "source": [
        "# now filter out punctuations\n",
        "import string\n",
        "\n",
        "puncts = list(string.punctuation)\n",
        "puncts.extend([r'``', r'--', r'...', r\"''\"])\n",
        "# print(puncts)\n",
        "\n",
        "def filter_puncts(tokens):\n",
        "  filtered_tokens = []\n",
        "  for t in tokens:\n",
        "    if t not in puncts:\n",
        "      filtered_tokens.append(t)\n",
        "  return filtered_tokens\n",
        "\n",
        "roxbury_unpunctuated = filter_puncts(roxbury_lower)\n",
        "print(roxbury_unpunctuated[:1000])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zRcAYO3PQPhV",
        "outputId": "aa21dfbb-1320-4a23-fd70-bdf2cf2b21c3"
      },
      "outputs": [],
      "source": [
        "# filter out stopwords\n",
        "def remove_stopwords(tokens):\n",
        "  tokens_nonstop = []\n",
        "  for t in tokens:\n",
        "    if t not in stop_words:\n",
        "      tokens_nonstop.append(t)\n",
        "  return tokens_nonstop\n",
        "\n",
        "roxbury_nonstop = remove_stopwords(roxbury_unpunctuated)\n",
        "print(roxbury_nonstop[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-X4VEkESCFd"
      },
      "source": [
        "more will be removed after converting to lowercase, but I'm not sure I want to do this yet because want to preserve sentence boundaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKppT3ebSdTk"
      },
      "source": [
        "First though we can remove all numbers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ir-GyU8rR_N-",
        "outputId": "98898a4b-204f-482e-bbb8-00bc6ed7d7f0"
      },
      "outputs": [],
      "source": [
        "def remove_numbers(tokens):\n",
        "  filtered_tokens = []\n",
        "  for t in tokens:\n",
        "    if t.isalpha():\n",
        "      filtered_tokens.append(t)\n",
        "  return filtered_tokens\n",
        "\n",
        "roxbury_alpha = remove_numbers(roxbury_nonstop)\n",
        "print(roxbury_alpha[:1000])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ie-pjspZTW0z"
      },
      "source": [
        "# Now we'll rejoin the text so we can sentence tokenize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 183
        },
        "id": "JjEpxxhfS3NU",
        "outputId": "c9b4c453-b0d3-4426-a1b1-97185eb7a88e"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize.punkt import PunktSentenceTokenizer\n",
        "\n",
        "sentence_tokenizer = PunktSentenceTokenizer()\n",
        "roxbury_sentences = sentence_tokenizer.sentences_from_tokens(tokens=roxbury_alpha)\n",
        "print(roxbury_sentences[:100])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qDbVnzmzT4DQ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "spacy_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
