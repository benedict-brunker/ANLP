{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gdown\n",
      "  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting beautifulsoup4 (from gdown)\n",
      "  Downloading beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting filelock (from gdown)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: requests[socks] in c:\\users\\bened\\anaconda3\\envs\\spacy_env\\lib\\site-packages (from gdown) (2.32.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\bened\\anaconda3\\envs\\spacy_env\\lib\\site-packages (from gdown) (4.66.5)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4->gdown)\n",
      "  Downloading soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\bened\\anaconda3\\envs\\spacy_env\\lib\\site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\bened\\anaconda3\\envs\\spacy_env\\lib\\site-packages (from requests[socks]->gdown) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\bened\\anaconda3\\envs\\spacy_env\\lib\\site-packages (from requests[socks]->gdown) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\bened\\anaconda3\\envs\\spacy_env\\lib\\site-packages (from requests[socks]->gdown) (2024.8.30)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\bened\\anaconda3\\envs\\spacy_env\\lib\\site-packages (from tqdm->gdown) (0.4.6)\n",
      "Downloading gdown-5.2.0-py3-none-any.whl (18 kB)\n",
      "Downloading beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Downloading soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, PySocks, filelock, beautifulsoup4, gdown\n",
      "Successfully installed PySocks-1.7.1 beautifulsoup4-4.12.3 filelock-3.16.1 gdown-5.2.0 soupsieve-2.6\n"
     ]
    }
   ],
   "source": [
    "! pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdrive_url = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle \n",
    "\n",
    "with open(r\"C:\\Users\\bened\\DataScience\\ANLP\\AT2\\preprocessed_data\\processed_screenplays_final.pkl\", \"rb\") as f:\n",
    "    data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>imdbid</th>\n",
       "      <th>title</th>\n",
       "      <th>akas</th>\n",
       "      <th>year</th>\n",
       "      <th>metascore</th>\n",
       "      <th>imdb user rating</th>\n",
       "      <th>number of imdb user votes</th>\n",
       "      <th>awards</th>\n",
       "      <th>opening weekend</th>\n",
       "      <th>producers</th>\n",
       "      <th>...</th>\n",
       "      <th>casting directors</th>\n",
       "      <th>cast</th>\n",
       "      <th>countries</th>\n",
       "      <th>age restrict</th>\n",
       "      <th>plot</th>\n",
       "      <th>plot outline</th>\n",
       "      <th>keywords</th>\n",
       "      <th>genres</th>\n",
       "      <th>taglines</th>\n",
       "      <th>synopsis</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>120770</td>\n",
       "      <td>A Night at the Roxbury</td>\n",
       "      <td>Une nuit au Roxbury (France), Movida en el Rox...</td>\n",
       "      <td>1998</td>\n",
       "      <td>26</td>\n",
       "      <td>6</td>\n",
       "      <td>56537</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States:</td>\n",
       "      <td>Marie Cantin, Erin Fraser, Amy Heckerling, Ste...</td>\n",
       "      <td>...</td>\n",
       "      <td>Jeff Greenberg</td>\n",
       "      <td>Will Ferrell, Chris Kattan, Raquel Gardner, Vi...</td>\n",
       "      <td>United States</td>\n",
       "      <td>Argentina:13, Australia:M, Brazil:14, Canada:P...</td>\n",
       "      <td>Two dim-witted brothers dream of owning their ...</td>\n",
       "      <td>The Roxbury Guys, Steve and Doug Butabi, want ...</td>\n",
       "      <td>woman-on-top, nightclub, car-accident, 1990s, ...</td>\n",
       "      <td>Comedy, Music, Romance</td>\n",
       "      <td>Score!</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>132512</td>\n",
       "      <td>At First Sight</td>\n",
       "      <td>Sight Unseen (United States), Premier regard (...</td>\n",
       "      <td>1999</td>\n",
       "      <td>40</td>\n",
       "      <td>6</td>\n",
       "      <td>12922</td>\n",
       "      <td>NaN</td>\n",
       "      <td>United States:</td>\n",
       "      <td>Rob Cowan, Roger Paradiso, Irwin Winkler</td>\n",
       "      <td>...</td>\n",
       "      <td>Kerry Barden, Billy Hopkins, Suzanne Smith</td>\n",
       "      <td>Val Kilmer, Mira Sorvino, Kelly McGillis, Stev...</td>\n",
       "      <td>United States</td>\n",
       "      <td>Argentina:13, Australia:M, Canada:PG::(Alberta...</td>\n",
       "      <td>A blind man has an operation to regain his sig...</td>\n",
       "      <td>First Sight is true to the title from start to...</td>\n",
       "      <td>visual-agnosia, brother-sister-relationship, r...</td>\n",
       "      <td>Drama, Romance</td>\n",
       "      <td>Only Love Can Bring You To Your Senses., Scien...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>118661</td>\n",
       "      <td>The Avengers</td>\n",
       "      <td>Chapeau melon et bottes de cuir (France), Mit ...</td>\n",
       "      <td>1998</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>40784</td>\n",
       "      <td>FMCJ Award 1998, Golden Reel Award 1999, Razzi...</td>\n",
       "      <td>United States: $10,305,957, 16 Aug 1998</td>\n",
       "      <td>Susan Ekins, Jerry Weintraub</td>\n",
       "      <td>...</td>\n",
       "      <td>Susie Figgis</td>\n",
       "      <td>Ralph Fiennes, Uma Thurman, Sean Connery, Patr...</td>\n",
       "      <td>United States</td>\n",
       "      <td>Argentina:13, Australia:PG, Brazil:10, Canada:...</td>\n",
       "      <td>Two British Agents team up to stop Sir August ...</td>\n",
       "      <td>British Ministry Agent John Steed, under direc...</td>\n",
       "      <td>good-versus-evil, heroine, evil-man, villain, ...</td>\n",
       "      <td>Action, Adventure, Sci-Fi, Thriller</td>\n",
       "      <td>Mrs. Peel, we're needed., Extraordinary crimes...</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>215545</td>\n",
       "      <td>Bamboozled</td>\n",
       "      <td>The Very Black Show (France), It's Showtime (G...</td>\n",
       "      <td>2000</td>\n",
       "      <td>54</td>\n",
       "      <td>6</td>\n",
       "      <td>10373</td>\n",
       "      <td>Golden Berlin Bear 2001, Black Reel 2001, Imag...</td>\n",
       "      <td>United States:</td>\n",
       "      <td>Jon Kilik, Spike Lee, Kisha Imani Cameron</td>\n",
       "      <td>...</td>\n",
       "      <td>Aisha Coley</td>\n",
       "      <td>Damon Wayans, Savion Glover, Jada Pinkett Smit...</td>\n",
       "      <td>United States</td>\n",
       "      <td>Australia:MA, Finland:K-15, France:Tous public...</td>\n",
       "      <td>A frustrated African-American TV writer propos...</td>\n",
       "      <td>Dark, biting satire of the television industry...</td>\n",
       "      <td>television-industry, african-american, referen...</td>\n",
       "      <td>Comedy, Drama, Music</td>\n",
       "      <td>Starring the great negroe actors</td>\n",
       "      <td>In a New York City residence, Pierre Delacroix...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118715</td>\n",
       "      <td>The Big Lebowski</td>\n",
       "      <td>El gran Lebowski (Spain), O Grande Lebowski (P...</td>\n",
       "      <td>1998</td>\n",
       "      <td>71</td>\n",
       "      <td>8</td>\n",
       "      <td>724388</td>\n",
       "      <td>Honorable Mention 1998, ACCA 1998, Golden Berl...</td>\n",
       "      <td>United States: $5,533,844, 08 Mar 1998</td>\n",
       "      <td>Tim Bevan, John Cameron, Ethan Coen, Eric Fell...</td>\n",
       "      <td>...</td>\n",
       "      <td>John S. Lyons</td>\n",
       "      <td>Jeff Bridges, John Goodman, Julianne Moore, St...</td>\n",
       "      <td>United States, United Kingdom</td>\n",
       "      <td>Argentina:16, Argentina:18::(cable rating), Au...</td>\n",
       "      <td>Jeff \"The Dude\" Lebowski, mistaken for a milli...</td>\n",
       "      <td>When \"the dude\" Lebowski is mistaken for a mil...</td>\n",
       "      <td>rug, nihilism, pornographer, bowling-alley, de...</td>\n",
       "      <td>Comedy, Crime, Sport</td>\n",
       "      <td>Hay quienes tratan de ganarse la vida sin move...</td>\n",
       "      <td>A tumbleweed rolls up a hillside just outside ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   imdbid                   title  \\\n",
       "0  120770  A Night at the Roxbury   \n",
       "1  132512          At First Sight   \n",
       "2  118661            The Avengers   \n",
       "3  215545              Bamboozled   \n",
       "4  118715        The Big Lebowski   \n",
       "\n",
       "                                                akas  year  metascore  \\\n",
       "0  Une nuit au Roxbury (France), Movida en el Rox...  1998         26   \n",
       "1  Sight Unseen (United States), Premier regard (...  1999         40   \n",
       "2  Chapeau melon et bottes de cuir (France), Mit ...  1998         12   \n",
       "3  The Very Black Show (France), It's Showtime (G...  2000         54   \n",
       "4  El gran Lebowski (Spain), O Grande Lebowski (P...  1998         71   \n",
       "\n",
       "   imdb user rating  number of imdb user votes  \\\n",
       "0                 6                      56537   \n",
       "1                 6                      12922   \n",
       "2                 3                      40784   \n",
       "3                 6                      10373   \n",
       "4                 8                     724388   \n",
       "\n",
       "                                              awards  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2  FMCJ Award 1998, Golden Reel Award 1999, Razzi...   \n",
       "3  Golden Berlin Bear 2001, Black Reel 2001, Imag...   \n",
       "4  Honorable Mention 1998, ACCA 1998, Golden Berl...   \n",
       "\n",
       "                           opening weekend  \\\n",
       "0                          United States:    \n",
       "1                          United States:    \n",
       "2  United States: $10,305,957, 16 Aug 1998   \n",
       "3                          United States:    \n",
       "4   United States: $5,533,844, 08 Mar 1998   \n",
       "\n",
       "                                           producers  ...  \\\n",
       "0  Marie Cantin, Erin Fraser, Amy Heckerling, Ste...  ...   \n",
       "1           Rob Cowan, Roger Paradiso, Irwin Winkler  ...   \n",
       "2                       Susan Ekins, Jerry Weintraub  ...   \n",
       "3          Jon Kilik, Spike Lee, Kisha Imani Cameron  ...   \n",
       "4  Tim Bevan, John Cameron, Ethan Coen, Eric Fell...  ...   \n",
       "\n",
       "                            casting directors  \\\n",
       "0                              Jeff Greenberg   \n",
       "1  Kerry Barden, Billy Hopkins, Suzanne Smith   \n",
       "2                                Susie Figgis   \n",
       "3                                 Aisha Coley   \n",
       "4                               John S. Lyons   \n",
       "\n",
       "                                                cast  \\\n",
       "0  Will Ferrell, Chris Kattan, Raquel Gardner, Vi...   \n",
       "1  Val Kilmer, Mira Sorvino, Kelly McGillis, Stev...   \n",
       "2  Ralph Fiennes, Uma Thurman, Sean Connery, Patr...   \n",
       "3  Damon Wayans, Savion Glover, Jada Pinkett Smit...   \n",
       "4  Jeff Bridges, John Goodman, Julianne Moore, St...   \n",
       "\n",
       "                       countries  \\\n",
       "0                  United States   \n",
       "1                  United States   \n",
       "2                  United States   \n",
       "3                  United States   \n",
       "4  United States, United Kingdom   \n",
       "\n",
       "                                        age restrict  \\\n",
       "0  Argentina:13, Australia:M, Brazil:14, Canada:P...   \n",
       "1  Argentina:13, Australia:M, Canada:PG::(Alberta...   \n",
       "2  Argentina:13, Australia:PG, Brazil:10, Canada:...   \n",
       "3  Australia:MA, Finland:K-15, France:Tous public...   \n",
       "4  Argentina:16, Argentina:18::(cable rating), Au...   \n",
       "\n",
       "                                                plot  \\\n",
       "0  Two dim-witted brothers dream of owning their ...   \n",
       "1  A blind man has an operation to regain his sig...   \n",
       "2  Two British Agents team up to stop Sir August ...   \n",
       "3  A frustrated African-American TV writer propos...   \n",
       "4  Jeff \"The Dude\" Lebowski, mistaken for a milli...   \n",
       "\n",
       "                                        plot outline  \\\n",
       "0  The Roxbury Guys, Steve and Doug Butabi, want ...   \n",
       "1  First Sight is true to the title from start to...   \n",
       "2  British Ministry Agent John Steed, under direc...   \n",
       "3  Dark, biting satire of the television industry...   \n",
       "4  When \"the dude\" Lebowski is mistaken for a mil...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  woman-on-top, nightclub, car-accident, 1990s, ...   \n",
       "1  visual-agnosia, brother-sister-relationship, r...   \n",
       "2  good-versus-evil, heroine, evil-man, villain, ...   \n",
       "3  television-industry, african-american, referen...   \n",
       "4  rug, nihilism, pornographer, bowling-alley, de...   \n",
       "\n",
       "                                genres  \\\n",
       "0               Comedy, Music, Romance   \n",
       "1                       Drama, Romance   \n",
       "2  Action, Adventure, Sci-Fi, Thriller   \n",
       "3                 Comedy, Drama, Music   \n",
       "4                 Comedy, Crime, Sport   \n",
       "\n",
       "                                            taglines  \\\n",
       "0                                             Score!   \n",
       "1  Only Love Can Bring You To Your Senses., Scien...   \n",
       "2  Mrs. Peel, we're needed., Extraordinary crimes...   \n",
       "3                   Starring the great negroe actors   \n",
       "4  Hay quienes tratan de ganarse la vida sin move...   \n",
       "\n",
       "                                            synopsis  \n",
       "0                                                NaN  \n",
       "1                                                NaN  \n",
       "2                                                NaN  \n",
       "3  In a New York City residence, Pierre Delacroix...  \n",
       "4  A tumbleweed rolls up a hillside just outside ...  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "\n",
    "root_path = r'C:\\\\Users\\bened\\DataScience\\ANLP\\AT2'\n",
    "\n",
    "meta_df = pd.read_csv(f'{root_path}\\\\movie_meta_data.csv')\n",
    "meta_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def preprocess_for_phrases(data):\n",
    "\n",
    "    labeled_sentences = {}\n",
    "\n",
    "    for screenplay_idx, tokens in data.items():\n",
    "\n",
    "        current_sentence = []\n",
    "        processed_line = []\n",
    "        line_label = None\n",
    "\n",
    "        for token in tokens:\n",
    "            \n",
    "            # parse label\n",
    "            if token.startswith('@'):\n",
    "                line_label = np.int8(token[1:])\n",
    "                labeled_sentences[line_label] = []\n",
    "\n",
    "            # skip if token contains non-word chars \n",
    "            elif re.search(r'\\W', token) and token not in {'.', '\\n'}:\n",
    "                continue\n",
    "\n",
    "            elif token == ':':\n",
    "                continue\n",
    "\n",
    "            # skip over '-' \n",
    "            elif token == '-':\n",
    "                continue\n",
    "            \n",
    "            elif token == '.':\n",
    "                if current_sentence:\n",
    "                    processed_line.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            \n",
    "            elif token == '\\n':\n",
    "                if current_sentence:\n",
    "                    processed_line.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            \n",
    "            else:\n",
    "                current_sentence.append(token)\n",
    "        \n",
    "        if current_sentence:\n",
    "            processed_line.append(current_sentence)\n",
    "        \n",
    "        if line_label is not None:\n",
    "            labeled_sentences[line_label].extend(processed_line)\n",
    "        \n",
    "    return labeled_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@1', 'roxbury', '\\n ', 'write', 'steve', 'koren', 'ferrell', 'chris', 'kattan', 'june']\n"
     ]
    }
   ],
   "source": [
    "print(data[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_to_sents(dict):\n",
    "\n",
    "    sentences_dict = {}\n",
    "    \n",
    "    for key, value in dict.items():\n",
    "        # list of sentences of each value\n",
    "        sentences = []\n",
    "        current_sentence = []\n",
    "\n",
    "        for t in value:\n",
    "            if t.strip() == '\\n':\n",
    "                if current_sentence:\n",
    "                    sentences.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            else:\n",
    "                current_sentence.append(t)\n",
    "            \n",
    "        if current_sentence:\n",
    "            sentences.append(current_sentence)\n",
    "\n",
    "        sentences_dict[key] = sentences\n",
    "    \n",
    "    return sentences_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_data = back_to_sents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_phrases(sents_data):\n",
    "    sentences =  []\n",
    "    for key, sent_list in sents_data.items():\n",
    "        sentences.extend(sent_list)\n",
    "    return sentences \n",
    "\n",
    "sents = prep_phrases(sents_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = phrases.Phrases(\n",
    "    sentences=sents,\n",
    "    connector_words=phrases.ENGLISH_CONNECTOR_WORDS\n",
    ")\n",
    "bigrams_dict = bigrams.export_phrases()\n",
    "bigrams_df = pd.DataFrame.from_dict(\n",
    "    bigrams_dict, \n",
    "    orient='index'\n",
    ")\n",
    "bigrams_df.reset_index(inplace=True)\n",
    "bigrams_df.columns = [\"phrase\", \"score\"]\n",
    "bigrams_df.sort_values('score', inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34782</th>\n",
       "      <td>there's_something_about_mary_p.</td>\n",
       "      <td>483308.155063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33870</th>\n",
       "      <td>window.addeventlistener_domcontentloade</td>\n",
       "      <td>353530.965278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31034</th>\n",
       "      <td>snacky_smores</td>\n",
       "      <td>303026.541667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19419</th>\n",
       "      <td>bombosity_knickety</td>\n",
       "      <td>296842.326531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19420</th>\n",
       "      <td>knickety_knackety</td>\n",
       "      <td>296842.326531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19396</th>\n",
       "      <td>hesitates_moment</td>\n",
       "      <td>10.004623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24882</th>\n",
       "      <td>bathtub_fill</td>\n",
       "      <td>10.003902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5267</th>\n",
       "      <td>street_singer</td>\n",
       "      <td>10.003385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35025</th>\n",
       "      <td>run_upstage</td>\n",
       "      <td>10.002196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4502</th>\n",
       "      <td>fellow_musician</td>\n",
       "      <td>10.001976</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>37377 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        phrase          score\n",
       "34782          there's_something_about_mary_p.  483308.155063\n",
       "33870  window.addeventlistener_domcontentloade  353530.965278\n",
       "31034                            snacky_smores  303026.541667\n",
       "19419                       bombosity_knickety  296842.326531\n",
       "19420                        knickety_knackety  296842.326531\n",
       "...                                        ...            ...\n",
       "19396                         hesitates_moment      10.004623\n",
       "24882                             bathtub_fill      10.003902\n",
       "5267                             street_singer      10.003385\n",
       "35025                              run_upstage      10.002196\n",
       "4502                           fellow_musician      10.001976\n",
       "\n",
       "[37377 rows x 2 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_model = bigrams.freeze()\n",
    "bigram_model.save(r'C:\\Users\\bened\\DataScience\\ANLP\\AT2\\models\\bigram_model2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold to 25\n",
    "\n",
    "bigrams_select = phrases.Phrases(\n",
    "    sentences=sents,\n",
    "    connector_words=phrases.ENGLISH_CONNECTOR_WORDS,\n",
    "    threshold=10,\n",
    "    min_count=10\n",
    ")\n",
    "bigrams_dict = bigrams_select.export_phrases()\n",
    "bigrams_df = pd.DataFrame.from_dict(\n",
    "    bigrams_dict, \n",
    "    orient='index'\n",
    ")\n",
    "bigrams_df.reset_index(inplace=True)\n",
    "bigrams_df.columns = [\"phrase\", \"score\"]\n",
    "bigrams_df.sort_values('score', inplace=True, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                phrase        score\n",
      "16421  there's_something_about_mary_p.  464129.2600\n",
      "15565                      zoobi_doobi  166367.5131\n",
      "16105                    durbar_court/  151513.2708\n",
      "11446                        kol_nidre  141674.7468\n",
      "11457                       yom_kippur  131376.6684\n",
      "              phrase    score\n",
      "4890     cellar_door  10.0050\n",
      "5815     poise_ready  10.0038\n",
      "6515     inches_long  10.0033\n",
      "11273  cloud_exhaust  10.0031\n",
      "11076     pile_trash  10.0002\n"
     ]
    }
   ],
   "source": [
    "bigrams_df['score'] = bigrams_df['score'].round(4)\n",
    "print(bigrams_df.head())\n",
    "print(bigrams_df.tail()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16421</th>\n",
       "      <td>there's_something_about_mary_p.</td>\n",
       "      <td>464129.2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15565</th>\n",
       "      <td>zoobi_doobi</td>\n",
       "      <td>166367.5131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16105</th>\n",
       "      <td>durbar_court/</td>\n",
       "      <td>151513.2708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11446</th>\n",
       "      <td>kol_nidre</td>\n",
       "      <td>141674.7468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11457</th>\n",
       "      <td>yom_kippur</td>\n",
       "      <td>131376.6684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4890</th>\n",
       "      <td>cellar_door</td>\n",
       "      <td>10.0050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5815</th>\n",
       "      <td>poise_ready</td>\n",
       "      <td>10.0038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6515</th>\n",
       "      <td>inches_long</td>\n",
       "      <td>10.0033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11273</th>\n",
       "      <td>cloud_exhaust</td>\n",
       "      <td>10.0031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11076</th>\n",
       "      <td>pile_trash</td>\n",
       "      <td>10.0002</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17288 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                phrase        score\n",
       "16421  there's_something_about_mary_p.  464129.2600\n",
       "15565                      zoobi_doobi  166367.5131\n",
       "16105                    durbar_court/  151513.2708\n",
       "11446                        kol_nidre  141674.7468\n",
       "11457                       yom_kippur  131376.6684\n",
       "...                                ...          ...\n",
       "4890                       cellar_door      10.0050\n",
       "5815                       poise_ready      10.0038\n",
       "6515                       inches_long      10.0033\n",
       "11273                    cloud_exhaust      10.0031\n",
       "11076                       pile_trash      10.0002\n",
       "\n",
       "[17288 rows x 2 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams_table = pd.concat([bigrams_df.head(10), bigrams_df.tail(10)])\n",
    "bigrams_table.to_csv(r\"C:\\Users\\bened\\DataScience\\ANLP\\AT2\\visualizations\\bigrams_table.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for phrase, score in bigrams_df.iterrows():\n",
    "    print(phrase, \",\", score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9545</th>\n",
       "      <td>kama_sutra</td>\n",
       "      <td>227269.906250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18712</th>\n",
       "      <td>deutsches_technikmuseum</td>\n",
       "      <td>223773.446154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11287</th>\n",
       "      <td>khor_kalba</td>\n",
       "      <td>222631.744898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13159</th>\n",
       "      <td>bryn_mawr</td>\n",
       "      <td>220382.939394</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9476</th>\n",
       "      <td>kol_nidre</td>\n",
       "      <td>204641.300866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15128</th>\n",
       "      <td>snacky_smores</td>\n",
       "      <td>202017.694444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16583</th>\n",
       "      <td>durbar_court/</td>\n",
       "      <td>202017.694444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14508</th>\n",
       "      <td>piggly_wiggly</td>\n",
       "      <td>202017.694444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4962</th>\n",
       "      <td>belo_quinto</td>\n",
       "      <td>194802.776786</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12349</th>\n",
       "      <td>kuala_lumpur</td>\n",
       "      <td>191385.184211</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        phrase          score\n",
       "9545                kama_sutra  227269.906250\n",
       "18712  deutsches_technikmuseum  223773.446154\n",
       "11287               khor_kalba  222631.744898\n",
       "13159                bryn_mawr  220382.939394\n",
       "9476                 kol_nidre  204641.300866\n",
       "15128            snacky_smores  202017.694444\n",
       "16583            durbar_court/  202017.694444\n",
       "14508            piggly_wiggly  202017.694444\n",
       "4962               belo_quinto  194802.776786\n",
       "12349             kuala_lumpur  191385.184211"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams_df.iloc[3:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>phrase</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4502</th>\n",
       "      <td>geel_piet</td>\n",
       "      <td>7.425097e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4815</th>\n",
       "      <td>nai_nai</td>\n",
       "      <td>2.094580e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4332</th>\n",
       "      <td>chattar_lal</td>\n",
       "      <td>1.049702e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3655</th>\n",
       "      <td>yom_kippur</td>\n",
       "      <td>1.032060e+07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5080</th>\n",
       "      <td>ibn_khaldun</td>\n",
       "      <td>7.491990e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1853</th>\n",
       "      <td>central_intelligence_agency</td>\n",
       "      <td>7.519960e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>hello_hello</td>\n",
       "      <td>7.518800e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>keen_interest</td>\n",
       "      <td>7.507000e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>677</th>\n",
       "      <td>cough_fit</td>\n",
       "      <td>7.505850e+01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2747</th>\n",
       "      <td>pot_plant</td>\n",
       "      <td>7.502670e+01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5369 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           phrase         score\n",
       "4502                    geel_piet  7.425097e+07\n",
       "4815                      nai_nai  2.094580e+07\n",
       "4332                  chattar_lal  1.049702e+07\n",
       "3655                   yom_kippur  1.032060e+07\n",
       "5080                  ibn_khaldun  7.491990e+06\n",
       "...                           ...           ...\n",
       "1853  central_intelligence_agency  7.519960e+01\n",
       "298                   hello_hello  7.518800e+01\n",
       "1256                keen_interest  7.507000e+01\n",
       "677                     cough_fit  7.505850e+01\n",
       "2747                    pot_plant  7.502670e+01\n",
       "\n",
       "[5369 rows x 2 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = phrases.Phrases(bigrams_select[sents], min_count=15, threshold=75)\n",
    "# trigram_model = phrases.Phraser(trigrams)\n",
    "trigrams_dict = trigrams.export_phrases()\n",
    "trigrams_df = pd.DataFrame.from_dict(\n",
    "    trigrams_dict, \n",
    "    orient='index'\n",
    ")\n",
    "trigrams_df.reset_index(inplace=True)\n",
    "trigrams_df.columns = [\"phrase\", \"score\"]\n",
    "trigrams_df['score'] = trigrams_df['score'].round(4)\n",
    "trigrams_df.sort_values('score', inplace=True, ascending=False)\n",
    "trigrams_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                phrase    score\n",
      "18283    tile_bathroom  10.0193\n",
      "6718          cry_loud  10.0191\n",
      "4145         thank_hon  10.0190\n",
      "17281         wag_head  10.0170\n",
      "504         hear_sound  10.0161\n",
      "12481         eat_chip  10.0158\n",
      "10161       duck_sight  10.0127\n",
      "4321     couple_minute  10.0118\n",
      "13044   hotel_employee  10.0103\n",
      "6057         cook_food  10.0089\n",
      "15900   unaware_danger  10.0081\n",
      "1072      roll_thunder  10.0072\n",
      "7325         watch_mtv  10.0064\n",
      "16681  job_application  10.0063\n",
      "6198      day_new_york  10.0046\n",
      "10172        dump_load  10.0035\n",
      "7006   absolutely_idea  10.0025\n",
      "18569         pig_tail  10.0018\n",
      "14930    sheer_curtain  10.0012\n",
      "15842      red_ferrari  10.0010\n"
     ]
    }
   ],
   "source": [
    "print(trigrams_df.tail(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "frozen_trigrams = trigrams.freeze() \n",
    "trigrams.save(r\"C:\\Users\\bened\\DataScience\\ANLP\\AT2\\models\\trigrams_final.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "['jonathan', 'ames', 'goldenrod', 'revision', 'green', 'revision', 'yellow', 'revision', 'revision', 'blue', 'revision', 'shooting', 'close', 'adult', 'man', 'mouth', 'water', 'joe', 'gulps', 'water', 'fill', 'air', 'stretch', 'smooth', 'surface', 'mist', 'droplet', 'moisture', 'form', 'break', 'miniature', 'rivulet', 'whisper', 'boy', 'voice', 'young', 'joe', 'count', 'sharp', 'inhale', 'clear', 'plastic', 'invert', 'face', 'joe', 'bag', 'suddenly', 'suck', 'hard', 'skin', 'distort', 'feature', 'inhuman', 'mask', 'boy', 'count', 'suspend', 'faint', 'beat', 'pulse', 'rise', 'replace', 'man', 'bag', 'balloon', 'cloud', 'man', 'breathe', 'slowly', 'plastic', 'bag', 'balloon', 'invert', 'boy', 'whisper', 'resume', 'man', 'never', 'blink', 'plastic', 'eye', 'unseee', 'rise', 'fall', 'mouth', 'nose', 'sign', 'life', 'titles', 'montage', 'glimpse', 'methodical', 'process', 'camera', 'tight', 'sound', 'heighten', 'fill', 'frame', 'passport', 'style', 'photograph', '16', 'years', 'old', 'smile', 'hint', 'lip', 'gloss', 'eye', 'shadow', 'lightly', 'latin', 'feature', 'image', 'appear', 'dance', 'flicker', 'inner', 'light', 'slowly', 'first', 'picture', 'darken', 'rapidly', 'distort', 'blacken', 'fire', 'smoke', 'erupt', 'girl', 'face', 'photograph', 'curl', 'flame', '1a', 'wider', 'burn', 'shard', 'photograph', 'drop', 'small', 'metal', 'trash', 'rise', 'smoke', 'staunch', 'heavy', 'bind', 'book', 'dump', 'rim', 'trash', 'baseball', 'cap', 'silver', 'duct', 'tape', 'bundle', 'heavy', 'duty', 'plastic', 'zip', 'tie', 'close', 'ceiling', 'smoke', 'detector', 'condom', 'stretch', 'pull', 'free', 'blacken', 'scrap', 'trash', 'shake', 'laundry', 'bag', 'flip', 'form', 'prepay', 'cell', 'phone', 'strain', 'gloved', 'hand', 'crack', 'hinge', 'shatter', 'two', 'half', 'go', 'laundry', 'bag', 'peen', 'hammer', 'lie', 'length', 'bathroom', 'hand', 'towel', 'fresh', 'blood', 'head', 'hammer', 'wipe', 'towel', 'item', 'drop', 'laundry', 'bag', 'bag', 'close', 'tight', '2nd', 'low', 'angle', 'man', 'torso', 'emerge', 'laundry', 'bag', 'dangle', 'hip', 'make', 'way', 'stealthily', 'dingy', 'corridor', 'round', 'corner', 'woman', 'hone', 'view', 'man', 'hang', 'back', 'back', 'woman', 'tweak', 'barely', 'skirt', 'strut', 'away', 'wait', 'elevator', 'hotel', 'clean', 'service', 'trolley', 'leave', 'door', 'service', 'stairwell', 'man', 'dump', 'bag', 'trash', 'slip', 'door', 'check', 'desk', 'unmanned', 'clock', 'headline', 'wkrc', 'local', 'news', 'man', 'move', 'silently', 'stairwell', 'fire', 'door', 'head', 'shroud', 'jacket', 'hood', 'portable', 'tv', 'flicker', 'room', 'glimpse', 'fat', 'belly', 'receptionist', 'front', 'man', 'glide', 'stop', 'short', 'glass', 'door', 'lead', 'street', 'police', 'light', 'kick', 'outside', 'pause', 'one', 'side', 'door', 'back', 'press', 'wall', 'door', 'sliver', 'street', 'activity', 'idle', 'yard', 'hotel', 'cop', 'sidewalk', 'size', 'local', 'dreg', 'hotel', 'receptionist', 'move', 'unseen', 'item', 'food', 'lap', 'mouth', 'tension', 'rise', 'street', 'cop', 'hand', 'drop', 'holster', 'desk', 'phone', 'ring', 'receptionist', 'begin', 'stand', 'eye', 'still', 'glue', 'screen', 'swiftly', 'double', 'back', 'lobby', 'service', 'stairwell', 'floor', 'cincinnati', 'night', 'man', 'slip', 'open', 'elevator', 'corridor', 'fire', 'exit', 'door', 'far', 'end', 'passage', 'hotel', 'see', 'cincinnati', 'back', 'high', 'fire', 'exit', 'swing', 'open', 'man', 'slip', 'impassive', 'face', 'hard', 'mask', 'like', 'mournful', 'eye', 'joe', 'plaid', 'shirt', 'jacket', 'baseball', 'cap', 'raise', 'hood', 'joe', 'pov', 'ground', 'dozens', 'dead', 'bird', 'litter', 'alleyway', 'exit', 'ahead', 'ease', 'mouth', 'alleyway', 'slow', 'halt', 'joe', 'turn', 'opposite', 'direction', 'shadow', 'step', 'arm', 'aloft', 'brandish', 'blackjack', 'joe', 'sense', 'behind', 'presence', 'life', 'come', 'violence', 'assailant', 'alone', 'alley', 'vomit', 'passenger', 'door', 'yellow', 'cab', 'slam', 'shut', 'reveal', 'emblem', 'cab', 'cincinnati', 'airport', 'tired', 'line', 'mouth', 'elderly', 'cab', 'driver', 'reply', 'grunt', 'stare', 'ahead', 'joe', 'pov', 'rear', 'view', 'mirror', 'cab', 'driver', 'lip', 'dip', 'view', 'strobe', 'streetlight', 'driver', 'appear', 'silently', 'mouth', 'never', 'really', 'word', 'appear', 'screen', 'title', 'fade', 'driver', 'appear', 'speak', 'spin', 'pad', 'floor', 'polisher', 'reverberate', 'empty', 'arrival', 'hall', 'wide', 'lone', 'teenage', 'asleep', 'bench', 'head', 'prop', 'knapsack', 'bent', 'double', 'water', 'fountain', 'joe', 'watch', 'gleam', 'concourse', 'face', 'shade', 'brim', 'take', 'mouthful', 'water', 'la', 'arrival', 'hall', 'moment', 'later', 'joe', 'finger', 'tap', 'digit', 'pay', 'phone', 'mouth', 'press', 'receiver', 'do', 'hang', 'suburban', 'neighborhood', 'sleep', 'dead', 'night', 'joe', 'step', 'nyc', 'yellow', 'cab', 'wait', 'kerb', 'watch', 'turn', 'view', 'far', 'intersection', 'end', 'avenue', 'slinking', 'footpath', 'block', 'house', 'joe', 'approach', 'entrance', 'house', 'end', 'row', 'stoop', 'remove', 'thin', 'wooden', 'baton', 'door', 'sill', 'slip', 'drainpipe', '6a', 'take', 'care', 'make', 'sound', 'turn', 'key', 'lock', 'ease', 'door', 'open', 'faint', 'sound', 'footstep', 'clink', 'glass', 'still', 'door', 'joe', 'attention', 'fire', 'escape', 'block', 'overlook', 'house', 'skinny', 'teenage', 'crouch', '3rd', 'floor', 'fire', 'gantry', 'body', 'freeze', 'hold', 'smoke', 'joint', 'hand', '2nd', 'climb', 'open', 'apartment', 'window', 'join', 'two', 'beer', 'bottle', 'hand', 'moises', 'joe', 'look', 'hold', 'beat', 'expression', 'melt', 'recognition', 'fear', 'let', 'go', 'lungful', 'joe', 'turn', 'away', 'close', 'door', 'house', 'joe', 'stand', 'doorway', 'living', 'room', 'cold', 'light', 'dance', 'face', 'tv', 'sound', 'blare', 'bad', 'beautiful', 'tcm', 'joe', 'mother', '80s', 'house', 'dress', 'slipper', 'asleep', 'chair', 'joe', 'marine', 'corps', 'dress', 'uniform', 'tv', 'reflect', 'glass', 'slump', 'nose', 'joe', 'flick', 'tv', 'plunge', 'room', 'stillness', 'remove', 'glass', 'care', 'set', 'table', 'hear', 'aid', 'house', 'joe', 'stand', 'foot', 'stair', 'mother', 'obscure', 'view', 'bannister', 'baby', 'step', 'view', 'joe', 'step', 'back', 'let', 'pass', 'take', 'stair', 'move', 'snail', 'pace', 'one', 'step', 'next', 'joe', 'watch', 'tedious', 'progress', 'upstairs', '7a', 'joe', 'place', 'mother', 'delicately', 'bed', 'draw', 'cover', 'window', 'ajar', 'curtain', 'billow', 'chill', 'blow', 'room', 'joe', 'pull', 'window', 'sharp', 'creak', 'bed', 'freeze', 'turn', 'back', 'window', 'rest', 'handle', 'beat', 'one', 'fall', 'swoop', 'joe', 'window', 'shut', 'break', 'room', 'never', 'look', 'back', 'bed', 'joe', 'bedroom', 'mother', 'house', 'joe', 'lie', 'rigid', 'top', 'bed', 'cover', 'eye', 'pop', 'open', 'move', 'try', 'scream', 'sound', 'come', 'episode', 'night', 'terror', 'sound', 'dripping', 'water', 'struggle', 'let', 'moan', 'awake', 'sweating', 'joe', 'sit', 'bed', 'see', 'vicious', 'bruise', 'leave', 'shoulder', 'body', 'network', 'scar', 'insistently', 'one', 'clean', 'move', 'joe', 'bolt', 'bed', 'door', 'house', 'watch', 'mother', 'bedroom', 'bathroom', 'door', 'ajar', 'mother', 'pewter', 'hair', 'reach', 'waist', 'wrap', 'robe', 'give', 'hand', 'basin', 'rinse', 'throw', 'head', 'back', 'young', 'woman', 'long', 'silvery', 'rope', 'joe', 'look', 'floor', 'bathroom', 'awash', 'soapy', 'water', 'joe', 'sigh', 'sight', 'soapy', 'water', 'cumulate', 'floor', 'hand', 'knee', 'work', 'towel', 'soak', 'soapy', 'water', 'tile', 'radio', 'loud', 'joe', 'stand', 'kitchen', 'wear', 'suit', 'tie', 'mother', 'wrinkle', 'hand', 'smooth', 'unkempt', 'hair', 'hand', 'move', 'beard', 'also', 'attempt', 'groom', 'hide', 'beautiful', 'face', 'straighten', 'adjust', 'tie', 'joe', 'face', 'mildly', 'annoyed', 'joe', 'mother', 'frail', 'hand', 'crack', 'two', 'egg', 'fry', 'pan', 'sizzle', 'oil', 'mute', 'drone', 'radio', 'deaf', 'ear', 'oblivious', 'noise', 'poke', 'middle', 'fry', 'egg', 'destroy', 'perfect', 'sunny', 'side', 'turn', 'annoyed', '9a', 'smile', 'help', 'laugh', 'shoos', 'away', 'turn', 'stove', 'joe', 'sit', 'kitchen', 'table', 'face', 'mother', 'back', 'mother', 'glance', 'smile', 'turn', 'back', 'joe', 'pov', 'mother', 'frail', 'hand', 'struggle', 'lift', 'weight', 'fry', 'pan', 'stove', 'house', 'early', 'morning', 'later', 'joe', 'mother', 'sit', 'table', 'silence', 'breakfast', 'finish', 'radio', 'still', 'drone', 'look', 'question', 'widen', 'eye', 'ask', 'word', 'tilt', 'head', 'raise', 'shoulder', 'eye', 'search', 'sympathy', 'deep', 'resignation', 'joe', 'mimic', 'gesture', 'throw', 'hand', 'mad', 'move', 'unsteadily', 'clear', 'plate', 'put', 'hand', 'stop', 'take', 'plate', 'move', 'sink', 'watch', 'shoulder', 'begin', 'wash', 'joe', 'bedroom', 'mother', 'house', 'a21', 'joe', 'sit', 'edge', 'bed', 'shirt', 'slip', 'one', 'arm', 'hold', 'ice', 'pack', 'deep', 'bruise', 'bloom', 'right', 'shoulder', 'frozen', 'hand', 'work', 'stress', 'ball', 'exercise', 'joe', 'joe', 'joe', 'visibly', 'tense', 'outside', 'b21', 'joe', 'stand', 'rest', 'forehead', 'wall', 'sound', 'toilet', 'flush', 'joe', 'move', 'away', 'resignation', '10a', 'live', 'blare', 'tv', 'joe', 'mother', 'watch', 'joe', 'sit', 'couch', 'close', 'mother', 'fall', 'asleep', 'tie', 'undone', 'suit', 'jacket', 'rest', 'chair', 'old', 'western', 'tv', 'joe', 'memory', 'pair', 'girl', 'foot', 'lie', 'dusty', 'ground', 'heel', 'intermittently', 'kick', 'earth', 'miniature', 'ditch', 'dig', 'sand', 'live', 'later', 'back', 'blind', 'sunlight', 'glint', 'frame', 'photograph', 'younger', 'joe', 'ceremonial', 'military', 'dress', 'sunrise', 'touch', 'live', 'room', 'wall', 'joe', 'mother', 'armchair', 'front', 'tv', 'joe', 'mother', 'house', 'day', 'live', 'joe', 'mother', 'stare', 'street', 'smile', 'face', 'turn', 'dark', 'memory', 'joe', 'bedroom', 'mother', 'house', 'joe', 'sit', 'upright', 'chair', 'drag', 'open', 'doorway', 'walk', 'cupboard', 'cover', 'head', 'foot', 'shoelace', 'untie', 'use', 'one', 'foot', 'lever', 'shoe', 'place', 'sock', 'foot', 'floorboard', 'ahead', 'floorboard', 'creak', 'place', 'weight', 'bare', 'patch', 'skin', 'joe', 'sock', 'reveal', 'old', 'pronounce', 'scar', 'joe', 'belt', 'loop', 'neck', 'end', 'belt', 'loop', 'clothing', 'rail', 'pull', 'tightly', 'belt', 'house', 'day', 'joe', 'mother', 'sing', 'adorable', 'clean', 'sign', 'letter', 'alphabet', '11a', 'frail', 'hand', 'move', 'slowly', 'joe', 'buff', 'machine', 'hold', 'silver', 'spoon', 'right', 'front', 'mother', 'mouth', 'exhale', 'back', 'spoon', 'joe', 'give', 'finish', 'rub', 'later', 'joe', 'prize', 'open', 'tin', 'heavily', 'tarnish', 'silverware', 'lie', 'bake', 'tray', 'dining', 'table', 'large', 'item', 'serve', 'spoon', 'ladle', 'joe', 'scatter', 'bicarbonate', 'unscrews', 'bottle', 'white', 'vinegar', 'pass', 'mother', 'joe', 'mother', 'hear', 'aid', 'tuck', 'ear', 'back', 'front', 'tv', 'blanket', 'knee', 'dining', 'table', 'joe', 'fit', 'last', 'immaculately', 'clean', 'silver', 'case', 'fork', 'slot', 'place', 'break', 'close', 'latched', 'next', 'morning', 'joe', 'stand', 'caddy', 'corner', 'look', 'window', 'check', 'watch', '9am', 'joe', 'bedroom', 'mother', 'house', 'later', 'joe', 'sit', 'edge', 'bed', 'wear', 'jean', 'sleeveless', 'shirt', 'hand', 'wrap', 'neck', 'pressure', 'apply', 'throat', 'face', 'beat', 'red', 'joe', 'joe', 'joe', 'let', 'deep', 'sigh', 'joe', 'verge', 'tear', '12a', 'joe', 'sob', 'baby', 'joe', 'take', 'baton', 'hiding', 'place', 'drainpipe', 'stoop', 'carefully', 'position', 'door', 'sill', 'tuck', 'casual', 'view', 'slip', 'desert', 'way', 'move', 'quickly', 'invisibly', 'hardly', 'see', 'enter', 'house', 'ever', 'joe', 'lean', 'wall', 'watch', 'street', 'bodega', 'always', 'open', 'joe', 'pov', 'stout', 'come', 'upstairs', 'window', 'peep', 'dirty', 'net', 'curtain', 'move', 'sight', 'check', 'watch', 'black', 'analogue', 'dial', 'issue', 'seconds', 'joe', 'pov', 'bodega', 'window', 'sign', 'read', 'egg', 'bacon', 'sandwich', 'removable', 'letter', 'cinema', 'sign', 'silhouette', 'stout', 'man', 'upstairs', 'reach', 'window', 'display', 'remove', 'sign', 'man', 'replace', 'sign', 'letter', 'read', 'joe', 'register', 'misspell', 'sign', 'cue', 'dead', 'drop', 'head', 'road', 'bodega', 'shop', 'empty', 'customer', 'crude', 'bell', 'ring', 'opening', 'door', 'tall', 'skinny', 'kid', 'spot', 'joe', 'enter', 'house', 'busie', 'rearrange', 'good', 'shelf', 'immediately', 'avert', 'gaze', 'joe', 'hand', 'shake', 'stack', 'tomato', 'can', 'fall', 'floor', 'stout', 'middle', 'aged', 'man', 'emerge', 'stock', 'room', 'angry', 'look', 'moises', 'joe', 'quickly', 'move', 'stockroom', 'angel', 'follow', 'angel', 'grab', 'small', 'step', 'stool', 'place', 'floor', 'climb', 'reach', 'broken', 'ceiling', 'tile', 'hand', 'fish', 'moment', 'pull', 'envelope', 'hand', 'joe', 'joe', 'open', 'envelope', 'take', 'bind', 'wad', 'skim', 'hand', 'back', 'immediately', 'push', 'shirt', 'top', 'pocket', 'man', 'call', 'see', 'right', 'away', 'small', 'army', 'cockroache', 'scoot', 'chip', 'floor', 'tile', 'two', 'man', 'son', 'tell', 'see', 'military', 'response', 'friend', 'coincidence', 'tell', 'see', 'hesitate', 'know', 'beat', 'mean', 'see', 'good', 'boy', 'joe', 'eye', 'bear', 'angel', 'clear', 'lie', 'joe', 'give', 'slight', 'shrug', 'resignation', 'close', 'eye', 'slowly', 'gently', 'almost', 'fall', 'asleep', 'stand', 'skim', 'wad', 'hand', 'look', 'sadness', 'angel', 'eye', 'take', 'severance', 'pay', 'know', 'day', 'joe', 'dead', 'drop', 'terminate', 'joe', 'exit', 'back', 'room', 'speak', 'move', 'liquid', 'way', 'custom', 'queens', 'plaza', 'joe', 'stand', 'platform', 'edge', 'baseball', 'cap', 'pull', 'low', 'platform', 'young', 'bruise', 'eye', 'stare', 'impassively', 'joe', 'rattle', 'approach', 'train', 'rise', 'volume', 'close', 'joe', 'foot', 'begin', 'step', 'yellow', 'line', 'mark', 'platform', 'safe', 'zone', 'mid', 'stride', 'foot', 'hover', 'momentarily', 'precipice', 'train', 'strobe', 'platform', 'deafen', 'battle', 'brake', 'metal', 'joe', 'go', 'manhattan', '38th', 'street', '8th', 'end', 'corridor', 'lobby', 'joe', 'walk', 'elevator', 'door', 'begin', 'close', 'view', 'suited', 'arm', 'appear', 'field', 'vision', 'hold', 'back', 'door', 'begin', 'slide', 'open', 'joe', 'ignore', 'step', 'neatly', 'side', 'view', 'accountant', 'type', 'slouch', 'elevator', 'man', 'hold', 'closing', 'door', 'back', 'scowl', 'gesture', 'ignore', 'joe', 'shade', 'eye', 'baseball', 'cap', 'hit', 'bottom', 'stairwell', 'accountant', 'shrug', 'elevator', 'door', 'push', 'shut', 'morning', 'man', 'head', 'tip', 'blood', 'soak', 'tissue', 'breathe', 'methodically', 'mouth', 'lift', 'blood', 'soak', 'tissue', 'joe', 'see', 'try', 'kill', 'today', 'wider', 'room', 'cluttered', 'ornate', 'floral', 'display', 'park', 'floor', 'filing', 'cabinet', 'laugh', 'cincinnati', 'kid', 'florist', 'joe', 'rub', 'bruised', 'shoulder', 'smile', 'reach', 'forward', 'grab', 'candy', 'desk', 'tone', 'shift', 'rid', 'angel', 'number', '11l', 'find', 'new', 'service', 'goldenrod', 'pages', 'little', 'throw', 'okay', 'problem', 'reach', 'rolodex', 'scoot', 'desk', 'big', 'fat', 'drop', 'blood', 'splash', 'index', 'card', 'call', 'directly', 'joe', 'eye', 'still', 'close', 'sigh', 'albert', 'votto', 'joe', 'play', 'candy', 'mccleary', 'continue', 'run', 'father', 'security', '80s', 'lose', 'touch', 'arraignment', 'poof', 'aneurysm', 'burst', 'brain', 'middle', 'white', 'wipe', 'last', 'blood', 'nose', 'half', 'asleep', 'close', 'eye', 'wife', 'votto', 'jr', 'wife', 'kill', 'couple', 'years', 'ago', 'good', 'look', 'politic', 'daughter', 'run', 'away', 'home', 'ever', 'since', 'pick', 'albany', 'police', 'time', 'problem', 'child', 'beat', 'fucking', 'go', 'sleep', 'bastard', 'joe', 'awake', 'call', 'morning', 'missing', 'weekend', 'want', 'cop', 'involve', 'williams', 'camp', 'much', 'heat', 'press', 'lead', 'get', 'anonymous', 'text', 'joe', 'barely', 'listen', 'uh', 'uh', 'beat', 'beat', 'want', 'meet', 'person', 'reach', 'seal', 'brown', 'envelope', 'fuck', 'kid', 'catch', 'joe', 'look', 'slide', 'envelope', 'forward', 'consult', 'sheet', 'paper', 'desk', 'behave', 'remember', 'joe', 'people', 'enjoy', 'gang', 'rape', 'flower', 'send', 'keep', 'john', 'really', 'tie', 'room', 'together', 'reference', 'photo', 'deceased', 'wife', 'love', 'flower', 'shit', 'bless', 'never', 'see', 'point', 'die', 'anyway', 'stink', 'place', 'indicate', 'brown', 'envelope', 'desk', 'joe', 'stand', 'take', 'brown', 'envelope', 'tap', 'desk', 'head', 'door', '4l', 'thank', 'john', 'two', 'dry', 'dock', 'thank', 'drive', 'harbour', 'fuck', 'firework', 'parade', 'steak', 'cold', 'beer', 'set', 'sun', 'whatcha', 'say', 'filp', 'bird', 'office', 'build', 'moment', 'later', '4l', 'foot', 'pound', 'rhythmically', 'step', 'joe', 'pov', 'rush', 'fire', 'door', 'foot', 'stair', 'low', 'end', 'his', 'rise', 'pitch', 'volume', 'lobby', 'lotte', 'check', 'watch', 'joe', 'sit', 'lobby', 'lotte', 'hotel', 'joe', 'notice', 'group', 'young', 'chinese', 'girl', 'laugh', 'take', 'selfie', 'wait', 'high', 'end', 'luggage', 'bellhop', 'come', 'put', 'luggage', 'cart', 'joe', 'take', 'notice', 'distinct', 'mole', 'upper', 'lip', 'one', 'girl', 'truck', 'nyc', 'docks', 'fbi', 'one', 'upper', 'lips', 'close', 'mouth', 'strain', 'wide', 'lobby', 'lotte', 'joe', 'odd', 'finery', 'ornate', 'lobby', 'move', 'purpose', 'elevator', 'old', 'school', 'bellhop', 'hold', 'door', 'open', 'clientele', 'joe', 'breeze', 'pull', 'peak', 'ballcap', 'low', 'step', 'neatly', 'door', 'service', 'stairwell', 'service', 'lotte', 'joe', 'make', 'way', 'head', 'tuck', 'low', 'face', 'obscure', 'gaze', 'cctv', 'camera', 'landing', 'lotte', 'young', 'male', '20s', 'open', 'double', 'door', 'huge', 'suite', 'sumptuously', 'decorate', 'original', 'artwork', 'wall', 'hard', 'high', 'back', 'dining', 'chair', 'place', 'face', 'sofa', 'stand', 'isolated', 'sea', 'thick', 'carpeting', '40s', 'sit', 'sofa', 'elegantly', 'dress', 'take', 'foot', 'bound', 'joe', 'albert', 'votto', 'offer', 'hand', 'joe', 'joe', 'joseph', 'delicately', 'close', 'double', 'door', 'leave', 'votto', 'joe', 'alone', 'joe', 'sweat', 'hard', 'walk', 'flight', 'joe', '21a', 'votto', 'joe', 'hand', 'firm', 'handshake', 'place', 'hand', 'top', 'joe', 'intimate', 'smile', 'warmly', 'thank', 'take', 'time', 'see', 'joe', 'beat', 'mccleary', 'say', 'good', 'business', 'grin', 'joe', 'still', 'breathe', 'heavy', 'walk', 'votto', 'release', 'grip', 'gesture', 'joe', 'sit', 'sit', 'one', 'know', 'mccleary', 'boy', 'trust', 'life', 'sleek', 'coffee', 'table', 'lie', 'two', 'man', 'joe', 'remove', 'baseball', 'cap', 'joe', 'nod', 'ex', 'fbi', 'iraq', 'die', 'one', 'right', 'deadpan', 'die', 'fight', 'good', 'war', 'sometimes', 'good', 'goldenrod', 'pages', 'decade', 'service', 'beat', 'fbi', 'sex', 'traffic', 'task', 'force', 'work', 'less', 'thing', 'phone', 'coffee', 'table', 'buzz', 'pick', 'drop', 'voicemail', 'stop', 'vibrate', 'snatch', 'skim', 'thumb', 'screen', 'state', 'capitol', 'poison', 'albany', 'political', 'stuff', 'stand', 'grasp', 'clear', 'plastic', 'file', 'place', 'front', 'joe', 'get', 'text', 'message', 'morning', 'joe', 'read', 'text', 'votto', 'pace', 'heart', 'daughter', 'west', '38th', 'street', 'live', 'let', 'know', 'lean', 'forward', 'leafs', 'loose', 'assortment', 'print', 'out', 'file', 'scan', 'family', 'photograph', 'pretty', 'little', 'rich', 'pony', 'extremely', 'attractive', 'woman', '40s', 'take', 'recent', 'hair', 'still', 'long', 'yes', 'beat', 'beautiful', 'hair', 'never', 'find', 'body', 'voice', 'waver', 'watch', 'votto', 'pick', 'pack', 'marlboros', 'arm', 'sofa', 'tear', 'seal', 'draw', 'cigarette', 'mouth', 'pat', 'search', 'light', 'take', 'cigarette', 'lip', 'look', 'pitifully', 'mervous', 'night', 'state', 'new', 'york', 'even', 'let', 'light', 'terrace', 'pick', 'phone', 'scroll', 'message', 'hand', 'joe', 'send', 'message', 'buy', 'phone', 'overlap', 'joe', 'know', 'send', 'text', 'throw', 'phone', 'away', 'police', 'close', 'investigation', 'leave', 'bury', 'empty', 'casket', 'beat', 'feel', 'goldenrod', 'pages', 'pick', 'one', 'photo', 'nina', 'joe', 'place', 'votto', 'phone', 'coffee', 'table', 'pace', 'turn', 'cigarette', 'pack', 'hand', 'back', 'child', 'joe', 'photograph', 'formally', 'pose', 'girl', 'wear', 'white', 'party', 'dress', 'votto', 'hold', 'embrace', 'eye', 'close', 'rest', 'crown', 'head', 'joe', 'select', 'early', 'selfie', 'girl', 'family', '1l', 'need', 'nod', 'joe', 'slip', 'shirt', 'top', 'pocket', 'daughter', 'take', 'medication', 'girl', 'often', 'keep', 'sedate', 'morphine', 'derivative', 'anti', 'depressants', 'mother', 'live', 'happen', 'let', 'give', 'number', 'phone', 'hand', 'phone', 'begin', 'pour', 'drink', 'brandy', 'decanter', 'go', 'senton', 'hotel', '27th', 'park', 'front', 'take', 'beat', '1l', 'find', 'joe', 'know', 'stare', 'ahead', 'nod', 'grateful', 'realize', 'name', 'nina', 'mention', 'car', 'rental', 'day', 'car', 'rental', 'hand', 'scarlet', 'false', 'nail', 'pass', 'set', 'key', 'window', 'joe', 'smile', 'charmingly', 'head', 'door', 'pass', 'complimentary', 'pastry', 'coffee', 'pick', 'napkin', 'pile', 'car', 'rental', 'lot', 'day', 'joe', 'approach', 'car', 'black', '4', 'door', 'cadillac', 'sedan', 'joe', 'pull', 'enterprise', 'lot', '38th', 'street', 'day', 'joe', 'pov', 'car', 'window', 'slow', 'crawl', 'size', 'west', 'keypad', 'intercom', 'security', 'camera', 'porch', '28', '28a', 'rental', 'car', 'midtown', 'rental', 'car', 'drive', 'entrance', 'ramp', 'turn', 'bottom', 'weave', 'park', 'car', 'joe', 'basket', 'hand', 'drop', 'surgical', 'adhesive', 'tape', 'plastic', 'bottle', 'rub', 'alcohol', 'aisle', 'joe', 'pick', 'tube', 'glue', 'ring', 'joe', 'item', 'make', 'removal', 'wipe', 'box', 'sanitary', 'napkin', 'last', 'basket', 'coke', 'bottle', 'mineral', 'water', 'joe', 'grab', 'hershel', 'bar', 'rack', 'cash', 'register', 'add', 'item', 'chinatown', 'restaurant', 'joe', 'slip', 'step', 'basement', 'entrance', 'front', 'restaurant', 'window', 'several', 'beat', 'joe', 'ascend', 'basement', 'step', 'carry', 'plastic', 'bag', 'emblazon', 'chinese', 'character', 'logo', 'electrical', 'good', 'vendor', 'apartment', 'build', 'joe', 'exit', 'service', 'stairwell', 'apartment', 'build', 'coke', 'dealer', 'hand', 'clear', 'plastic', 'vial', 'print', 'prescription', 'label', 'contain', 'white', 'pill', 'pass', 'joe', 'hand', 'exchange', 'cash', 'peen', 'hammer', 'joe', 'glance', 'hammer', 'manhattan', 'bustle', 'gaudy', 'light', 'silhouette', 'people', 'move', 'way', 'ant', 'side', 'walk', 'street', 'light', 'play', 'joe', 'face', 'green', 'yellow', 'carry', 'hardware', 'store', 'bag', 'eye', 'shade', 'baseball', 'cap', 'occasionally', 'catch', 'glint', 'color', 'light', 'never', 'blink', 'black', 'trunk', 'rental', 'car', 'swing', 'open', 'reveal', 'joe', 'standing', 'dump', 'tool', 'trunk', 'joe', 'shopping', 'lay', 'neatly', 'inside', 'lid', 'trunk', 'slam', 'shut', 'russian', 'steam', 'shadow', 'light', 'head', 'bow', 'eye', 'close', 'sweat', 'romanesque', 'mist', 'pot', 'belly', 'muscle', 'tattoo', 'joe', 'eye', 'half', 'open', 'half', 'close', 'cat', 'stare', 'tile', 'rumble', 'close', 'envelop', 'steam', 'cold', 'russian', 'joe', 'lie', 'surface', 'pool', 'us', 'military', 'joe', 'memory', 'sear', 'midday', 'sun', 'young', 'afghan', 'girl', 'walk', 'camera', 'hand', 'outstretche', 'young', 'joe', '20', 'fresh', 'faced', 'dress', 'army', 'fatigue', 'side', 'chainlink', 'fence', 'crudely', 'make', 'bench', 'drinking', 'coffee', 'eat', 'soldier', 'mill', 'dance', 'together', 'pop', 'play', 'park', 'vehicle', 'background', 'note', 'girl', 'reach', 'pocket', 'pull', 'bar', 'chocolate', 'girl', 'smile', 'candy', 'bar', 'hand', 'turn', 'joe', 'run', 'away', 'later', 'joe', 'look', 'lone', 'tree', 'girl', 'sit', 'shade', 'approach', 'stand', 'hold', 'hand', 'girl', 'shake', 'head', 'boy', 'shout', 'inaudible', 'girl', 'face', 'suddenly', 'aggressive', 'shake', 'head', 'try', 'hide', 'chocolate', 'fold', 'boy', 'pull', 'makarov', 'pistol', 'pocket', 'shoot', 'girl', 'stomach', 'snatch', 'chocolate', 'hand', 'run', 'slow', 'pool', 'blood', 'form', 'girl', 'bare', 'heel', 'kick', 'fast', 'furtively', 'dusty', 'ground', 'us', 'military', 'later', 'still', 'joe', 'pov', 'girl', 'leg', 'still', 'kick', 'dust', 'increase', 'sluggishness', 'girl', 'heel', 'intermittently', 'kick', 'earth', 'miniature', 'ditch', 'dig', 'sand', 'effort', 'retain', 'life', '30a', 'close', 'movement', 'heel', 'begin', 'falter', 'russian', 'joe', 'wash', 'hand', 'sink', 'mirror', 'notice', 'ghost', 'figure', 'cross', 'behind', 'water', 'run', 'shoulder', 'goldenrod', 'pages', 'rental', 'car', 'midtown', 'glove', 'flip', 'open', 'rub', 'alcohol', 'surgical', 'tape', 'make', 'wipe', 'sanitary', 'napkin', 'place', 'bottle', 'water', 'coke', 'slot', 'dash', 'mount', 'drink', 'holder', 'car', 'windscreen', 'joe', 'focus', 'sit', 'driver', 'seat', 'unsheathe', 'hammer', 'hand', 'sit', 'well', 'joe', 'slide', 'leave', 'hand', 'jacket', 'pocket', 'car', 'joe', 'rental', 'car', 'exit', 'parking', 'garage', 'rental', 'car', 'joe', 'eye', 'dark', 'inscrutable', 'shadow', 'baseball', 'cap', 'rainbow', 'hue', 'illumination', 'play', 'set', 'joe', 'pov', 'manhattan', 'street', 'late', 'night', 'nyc', '38th', 'st', 'night', 'joe', 'rental', 'car', 'turn', 'end', 'road', 'book', 'end', 'modern', 'apartment', 'building', 'one', '3rd', 'avenue', 'one', '2nd', 'avenue', 'rental', 'car', 'night', 'engine', 'note', 'drop', 'joe', 'slow', 'crawl', 'joe', 'pov', '3', 'storied', 'brownstone', 'street', 'brownstone', 'window', 'seal', 'shut', 'metal', 'curtain', 'insure', 'total', 'privacy', 'young', 'towel', 'boy', 'carry', 'laundry', 'sack', 'stair', 'punch', 'code', 'door', 'disappear', 'rental', 'later', 'joe', 'minute', 'hand', 'tick', 'see', 'high', 'end', 'black', 'town', 'car', 'pull', 'outside', 'brownstone', 'well', 'high', 'end', 'suit', 'walk', 'stair', 'driver', 'town', 'car', 'step', 'open', 'door', 'driver', 'get', 'back', 'car', 'pull', 'away', 'later', 'still', 'joe', 'sit', 'car', 'fugue', 'state', 'simultaneously', 'alert', 'peaceful', 'fbi', 'surveillance', 'car', 'nyc', 'docks', 'joe', 'young', 'joe', 'wear', 'standard', 'issue', 'fbi', 'nylon', 'jacket', 'sit', 'wheel', 'cruiser', 'rain', 'bear', 'windscreen', 'joe', 'pov', 'chain', 'link', 'fence', 'refrigerate', 'truck', 'sunflower', 'meat', 'corp', 'chinese', 'character', 'emblazon', 'flank', 'stand', 'quay', 'side', 'cab', 'door', 'open', 'engine', 'run', 'two', 'park', 'car', 'joe', 'truck', 'two', 'stand', 'discussion', 'deal', 'go', 'joe', 'check', 'watch', 'man', 'discussion', 'appear', 'inscrutable', 'ongoing', 'broken', 'burst', 'radio', 'static', 'wait', 'move', 'see', 'money', 'joe', 'grip', 'steering', 'wheel', 'knuckle', 'whiten', 'frustration', 'anger', 'distant', 'sound', 'truck', 'engine', 'run', 'rise', 'volume', 'joe', 'eye', 'shroud', 'darkness', 'sharp', 'street', 'light', 'throw', 'sad', 'shadow', 'raindrop', 'joe', 'rigid', 'face', 'jaw', 'tense', 'fume', 'continue', 'plume', 'truck', 'exhaust', '38th', 'windscreen', 'wiper', 'kick', 'automatically', 'judder', 'clear', 'shadow', 'accumulate', 'raindrop', 'joe', 'face', 'joe', 'eye', 'crawl', 'retreat', 'distort', 'water', 'pattern', 'wiper', 'wake', 'draw', 'close', 'focus', 'rear', 'view', 'mirror', 'joe', 'watch', 'towel', 'boy', 'thick', 'hooded', 'sweat', 'shirt', 'emerge', 'door', 'send', 'errand', 'joe', 'study', 'clearly', 'john', 'security', 'dart', 'car', '38th', 'st', 'night', 'towel', 'boy', 'shove', 'seat', 'car', 'joe', 'make', 'quick', 'work', 'cable', 'tie', 'duct', 'tape', 'bind', 'towel', 'boy', 'wrist', 'cable', 'tie', 'fasten', 'leg', 'together', 'ankle', 'knee', 'towel', 'boy', 'eye', 'start', 'focus', 'gasp', 'little', 'less', 'audibly', 'joe', 'ease', 'seat', 'exit', 'rear', 'door', 'return', 'driver', 'seat', 'face', 'forward', 'eye', 'lock', 'back', 'rear', 'view', 'mirror', 'many', 'security', 'mechanism', 'two', 'whisper', 'two', 'towel', 'boy', 'hesitates', 'refining', 'question', 'house', 'promise', 'kill', 'yes', 'towel', 'boy', 'hesitates', 'cunning', 'fear', 'speak', 'quickly', 'half', 'gasp', 'one', 'guy', 'guard', 'first', 'floor', 'camera', 'one', 'guy', 'second', 'floor', 'sit', 'hallway', 'joe', 'take', 'picture', 'nina', 'shirt', 'breast', 'pocket', 'hit', 'overhead', 'light', 'hold', 'front', 'towel', 'boy', 'face', 'playground', 'girl', 'playground', 'underage', 'girl', 'girl', 'picture', 'boy', 'mute', 'quietly', 'forcefully', 'other', 'american', 'speak', 'push', 'photo', 'close', 'tb', 'face', 'look', 'different', 'hair', 'third', 'floor', 'man', 'rich', 'guy', 'carry', 'key', 'code', 'front', 'door', 'joe', 'nod', 'one', 'movement', 'driver', 'seat', 'crosse', 'front', 'car', 'reappear', 'rear', 'passenger', 'door', 'goldenrod', 'pages', 'eye', 'widen', 'betrayal', 'follow', 'sequence', 'shoot', 'cctv', 'view', 'otherwise', 'state', 'walk', 'brothel', 'step', 'eye', 'shade', 'baseball', 'cap', 'punch', 'code', 'door', 'large', 'seated', 'sit', 'read', 'magazine', 'joe', 'immediately', 'hammer', 'strike', 'cheek', 'bone', 'send', 'wall', 'joe', 'take', 'two', 'step', 'time', 'guard', 'landing', 'joe', 'swing', 'hammer', 'two', 'hand', 'baseball', 'bat', 'send', 'man', 'sternum', 'double', 'joe', 'continue', 'corridor', 'thrown', 'joe', 'deliver', 'close', 'blow', 'man', 'continue', 'cctv', 'camera', 'linger', 'doorway', 'small', 'girl', 'wear', 'night', 'dress', 'wander', 'doorway', 'hallway', 'daze', 'cctv', 'angle', 'joe', 'approaches', 'close', 'door', 'open', 'enter', 'transition', 'room', 'dark', 'illuminate', 'uv', 'black', 'light', 'head', 'single', 'cot', 'bed', 'young', 'lie', 'top', 'white', 'sheet', 'joe', 'walk', 'skin', 'spectral', 'glow', 'joe', 'take', 'photograph', 'top', 'pocket', 'girl', 'eye', 'close', 'put', 'hand', 'cheek', 'reaction', 'gently', 'raise', 'one', 'eyelid', 'finger', 'thumb', 'moment', 'bright', 'blue', 'iris', 'anda', 'dilate', 'pupil', 'eye', 'roll', 'back', 'joe', 'take', 'aback', 'neck', 'arch', 'whole', 'body', 'begin', 'stiffen', 'joe', 'lean', 'lever', 'girl', 'thin', 'body', 'seat', 'position', 'arm', 'flop', 'forward', 'rag', 'doll', 'lift', 'chin', 'free', 'hand', 'snap', 'finger', 'loudly', 'ear', 'come', 'hear', 'look', 'say', 'name', 'nina', 'intake', 'breath', 'girl', 'head', 'loll', 'forward', 'firmly', 'cup', 'tiny', 'face', 'hand', 'eye', 'flicker', 'open', 'moment', 'lip', 'move', 'barely', 'audible', 'counting', 'skinny', 'diet', 'pill', 'blonde', 'silk', 'robe', 'artificial', 'tit', 'burst', 'room', 'cell', 'phone', 'hand', 'hysterical', 'eastern', 'european', 'accent', 'fuck', 'joe', 'advance', 'grab', 'elbow', 'violently', 'prize', 'cell', 'phone', 'grasp', 'get', 'dressed', 'joe', 'throw', 'nina', 'shock', 'get', 'bed', 'additional', 'clothing', 'room', 'pair', 'childish', 'frille', 'pop', 'sock', 'joe', 'reach', 'cell', 'phone', 'jammer', 'jacket', 'pocket', 'slide', 'switch', 'side', 'green', 'led', 'blink', 'put', 'wrench', 'jacket', 'joe', 'advance', 'big', 'sister', 'thrust', 'jacket', 'dress', 'nina', 'hand', 'dial', 'big', 'sister', 'cell', 'phone', 'clamp', 'department', 'glare', 'eye', 'understand', 'look', 'confused', 'call', 'connect', 'distant', 'female', 'voice', 'emergency', 'service', 'dispatch', 'hear', 'end', 'report', 'address', 'joe', 'shoot', 'stern', 'look', '41a', 'speak', 'joe', 'look', 'terrify', 'speak', 'joe', 'snatch', 'phone', 'joe', 'throw', 'floor', 'drive', 'heel', 'boot', 'smash', 'completely', 'leave', 'look', 'fear', 'eye', 'shake', 'head', 'go', 'shake', 'head', 'vigorously', 'punch', 'hand', 'repeatedly', 'head', 'paralysed', 'fear', 'move', 'stand', 'daze', 'silent', 'middle', 'room', 'joe', 'snatch', 'sheet', 'bed', 'wrap', 'nina', 'scoop', 'foot', 'aloft', 'joe', 'arm', 'nina', 'float', 'door', 'room', 'playground', 'barely', 'perceptible', 'whisper', 'pipe', 'music', '2nd', 'fall', 'still', 'fill', 'one', 'half', 'width', 'slump', 'knee', 'head', 'cowl', 'wall', 'stand', 'corridor', 'block', 'joe', 'passage', 'grasp', 'hunt', 'knife', 'hand', 'emblazon', 'eye', 'providence', 'tattoo', 'joe', 'gently', 'let', 'stare', 'implacably', 'rise', 'fall', 'naked', 'john', 'chest', 'eye', 'providence', 'stare', 'unblinke', 'face', '42a', 'sound', 'almighty', 'collision', 'body', 'joe', 'foot', 'step', 'sprawl', 'body', 'first', 'guard', 'march', 'front', 'door', 'seemingly', 'step', 'dull', 'techno', 'music', 'tiny', 'socked', 'foot', 'follow', 'pace', 'top', 'stoop', 'joe', 'peer', 'street', 'cop', 'car', 'six', 'minutes', 'elapse', 'joe', 'enter', 'girl', 'light', 'arm', 'move', 'quickly', 'rental', 'one', 'delicate', 'move', 'nina', 'front', 'seat', 'completely', 'step', 'trunk', 'car', 'joe', 'fling', 'open', 'rear', 'passenger', 'door', 'drag', 'towel', 'boy', 'shoulder', 'dump', 'sidewalk', 'storm', 'drain', 'opening', 'joe', 'crouch', 'kerb', 'fit', 'bloody', 'hammer', 'opening', 'drop', 'deeply', 'arm', 'reach', 'inside', 'cell', 'phone', 'jacket', 'pocket', 'follow', 'hammer', 'storm', 'drain', '38th', 'st', 'night', 'joe', 'rental', 'car', 'pull', 'away', 'towel', 'boy', 'lie', 'motionless', 'sidewalk', 'silence', 'reign', 'rental', 'car', 'passenger', 'window', 'cascade', 'reflection', 'rainwater', 'play', 'glass', 'stare', 'middle', 'distance', 'cop', 'car', 'pass', 'siren', 'light', 'blaring', 'close', 'continue', 'count', 'squeal', 'wet', 'tire', 'reverberate', 'around', 'empty', 'car', 'park', 'rental', 'pull', 'cruise', 'quiet', 'corner', 'engine', 'kill', 'car', 'park', 'moment', 'later', 'wound', 'rear', 'view', 'mirror', 'joe', 'mouth', 'clench', 'tight', 'sear', 'pain', 'jaw', 'set', 'tooth', 'grit', 'hard', 'continue', 'watch', 'fascination', 'joe', 'go', 'process', 'attend', 'wound', 'call', 'joe', 'hand', 'make', 'wipe', 'remove', 'blood', 'finger', 'joe', 'reach', 'coke', 'chocolate', 'hand', 'go', 'strain', 'open', 'ring', 'pull', 'abandon', 'half', 'cock', 'hand', 'joe', 'pop', 'open', 'take', 'home', 'nina', 'know', 'name', 'tell', 'name', 'alternate', 'sip', 'coke', 'square', 'chocolate', 'angry', 'sound', 'approach', 'car', 'clump', 'take', 'entrance', 'ramp', 'car', 'park', 'dry', 'echo', 'tire', 'concrete', 'joe', 'eye', 'fix', 'rear', 'view', 'mirror', 'headlight', 'anonymous', 'sedan', 'sweep', 'past', 'car', 'turn', 'ignition', 'key', 'enough', 'fire', 'electric', 'adjust', 'angle', 'door', 'mirror', 'mirror', 'sedan', 'pull', 'indistinct', 'figure', 'wheel', 'joe', 'wipe', 'discreet', 'porthole', 'condensation', 'building', 'window', 'reveal', 'man', 'black', 'tie', 'hurry', 'away', 'direction', 'exit', 'ramp', 'satisfied', 'joe', 'turn', 'attention', 'back', 'nina', 'eat', 'almost', 'chocolate', 'smear', 'lip', 'cheek', 'crudely', 'draw', 'smiley', 'face', 'adorn', 'passenger', 'window', 'burp', 'raise', 'hand', 'mouth', 'peer', 'surprised', 'guard', 'joe', 'face', 'wear', 'massive', 'grin', 'response', 'offer', 'joe', 'last', 'square', 'chocolate', 'packet', 'shake', 'head', 'unwavering', 'offer', 'last', 'piece', 'last', 'piece', 'world', 'take', 'put', 'mouth', 'close', 'eye', 'allow', 'relax', 'back', 'seat', 'distinctive', 'clunk', 'nina', 'release', 'latch', 'seat', 'belt', 'rasp', 'nylon', 'belt', 'retract', 'joe', 'look', 'go', 'stay', 'look', 'confusion', 'disappointment', 'passenger', 'door', 'car', 'wide', 'open', 'joe', 'stand', 'door', 'arm', 'fold', 'jacket', 'head', 'tip', 'forward', 'cautiously', 'steal', 'look', 'security', 'camera', 'perch', 'top', 'concrete', 'support', 'pillar', 'opposite', 'dip', 'eye', 'ground', 'sound', 'run', 'water', 'liquid', 'dark', 'pale', 'concrete', 'floor', 'parking', 'garage', 'run', 'door', 'car', 'joe', 'pool', 'boot', 'edge', 'side', 'avoid', 'nina', 'amass', '47', '47a', 'name', 'joe', 'finish', 'joe', 'nina', 'joe', 'car', 'long', 'wear', 'joe', 'jacket', 'lean', 'forward', 'grab', 'bottle', 'pill', 'shake', 'fail', 'open', 'bottle', 'joe', 'take', 'demonstrate', 'take', 'cap', 'put', 'cap', 'back', 'stash', 'bottle', 'reach', 'sit', 'back', 'seat', 'slip', 'nightie', 'shoulder', 'begin', 'hitch', 'tiny', 'thighs', 'meet', 'joe', 'gaze', 'look', 'mute', 'seduction', 'raise', 'forefing', 'lip', 'gesture', 'shhhh', 'joe', 'tell', 'mommy', 'wordlessly', 'lean', 'grasp', 'dangle', 'seat', 'belt', 'buckle', 'fling', 'arm', 'burie', 'face', 'chest', 'firmly', 'draw', 'seat', 'belt', 'back', 'untangle', 'arm', 'neck', 'break', 'moment', 'simple', 'signal', 'stop', 'slump', 'seat', 'head', 'roll', 'passenger', 'side', 'window', 'smiley', 'face', 'glass', 'begin', 'run', 'tear', 'clown', 'outside', 'glass', 'eyelid', 'draw', 'close', 'asleep', 'joe', 'wristwatch', 'nina', 'fast', 'asleep', 'rental', 'car', 'pull', 'right', 'front', 'pour', 'ipod', 'earbud', 'round', 'corner', 'head', 'hotel', 'hand', 'clutch', 'pair', 'well', 'wear', 'drumstick', 'beat', 'later', 'man', 'acne', 'scar', 'homebrew', 'neck', 'tattoo', 'emerge', 'throw', 'rucksack', 'shoulder', 'two', 'exchange', 'cross', 'path', 'disappear', 'street', 'joe', 'take', 'room', 'key', 'card', 'glove', 'compartment', 'bell', 'hop', 'guy', 'back', 'turn', 'struggle', 'ill', 'fitting', 'jacket', 'kitsch', 'lounge', 'musak', 'pipe', 'soothe', 'piano', 'low', 'rent', 'surrounding', 'joe', 'carry', 'nina', 'piggy', 'reception', 'booth', 'hair', 'stick', 'wet', 'face', 'still', 'doze', 'dirty', 'bare', 'foot', 'hang', 'loosely', 'joe', 'side', 'take', 'bottom', 'stair', 'joe', 'back', 'disappear', 'view', 'bell', 'hop', 'tap', 'rim', 'shoot', 'reception', 'desk', 'drumstick', 'oblivious', 'joe', 'entrance', 'lobby', 'fade', 'give', 'way', 'heighten', 'mix', 'sound', 'pass', 'closed', 'hotel', 'room', 'door', 'porn', 'move', 'furniture', 'muffle', 'cry', 'joe', 'carry', 'nina', 'rest', 'head', 'shoulder', 'hover', 'sleep', 'wakefulness', 'obscenely', 'childlike', 'harsh', 'lighting', 'whisper', 'barely', 'audible', 'mommy', 'go', 'away', 'gently', 'rock', 'shhhh', 'photograph', 'joe', 'take', 'votto', 'hold', 'finger', 'thumb', 'study', 'intently', 'overexposed', 'selfie', 'sit', 'one', 'armchair', 'room', 'joe', 'jacket', 'draw', 'shoulder', 'joe', 'reach', 'wet', 'wipe', 'smooth', 'chocolate', 'smear', 'face', 'uh', 'joe', 'wipe', 'nina', 'pull', 'face', 'joe', 'take', 'fresh', 'wipe', 'hone', 'nina', 'face', 'deliberately', 'go', 'eyed', 'burst', 'giggle', 'joe', 'smile', 'stand', 'throw', 'use', 'wipe', 'hotel', 'room', 'trash', 'family', 'portrait', 'photograph', 'cover', 'thumb', 'cover', 'thumb', 'lower', 'photo', 'eye', 'draw', 'tv', 'screen', 'flicker', 'away', 'sound', 'reach', 'remote', 'begin', 'channel', 'hop', '24h', 'newscast', 'accident', 'scene', 'midtown', 'manhattan', 'police', 'line', 'emergency', 'light', 'follow', 'montage', 'archive', 'shot', 'suit', 'middle', 'aged', 'man', 'give', 'speech', 'senate', 'name', 'williams', 'chyron', 'cry', 'man', '499a', 'face', 'bell', 'blood', 'spatter', 'two', 'men', 'raise', 'pistol', 'alpha', 'pair', 'twitch', 'gun', 'gesture', 'joe', 'living', 'area', 'room', 'seat', 'foot', 'bed', 'nina', 'turn', 'tv', 'holster', 'weapon', 'cross', 'nina', 'sling', 'shoulder', 'money', 'sack', 'head', 'door', 'j0e', 'tiny', 'foot', 'disappear', 'view', 'door', 'snap', 'shut', 'louder', 'muffle', 'door', 'joe', 'alpha', 'stand', 'joe', 'raise', 'gun', 'full', 'stock', 'instant', 'joe', 'propel', 'bed', 'lunge', 'full', 'length', 'coffee', 'table', 'planting', 'palm', 'top', 'surface', 'launch', 'floor', 'alpha', 'knee', 'cellphone', 'fly', 'alpha', 'hand', 'crash', 'forward', 'leg', 'pin', 'splinter', 'table', 'far', 'wall', 'move', 'rapid', 'assured', 'movement', 'teeter', 'man', 'three', 'solid', 'grasps', 'man', 'clothing', 'level', 'torso', 'go', 'control', 'wrist', 'weapon', 'man', 'try', 'vain', '9', 'mm', 'joe', 'ring', 'alpha', 'chop', 'back', 'joe', 'head', 'butt', 'gun', 'land', 'sharp', 'vicious', 'blow', 'flash', 'red', 'keep', 'climb', 'man', 'collapse', 'remnant', 'crush', 'coffee', 'table', 'joe', 'left', 'hand', 'wrist', 'hold', 'push', 'away', 'eye', 'screw', 'shut', 'pain', 'blow', 'head', 'unseee', 'snake', 'part', 'brain', 'take', 'impression', 'feel', 'deaden', 'gunshot', 'straight', 'ceiling', 'mirror', 'fracture', 'glass', 'right', 'length', 'see', 'last', 'struggle', 'fragmented', 'reflection', 'cubist', 'tableau', 'two', 'distorted', 'figure', 'deep', 'red', 'carpet', 'third', 'gunshot', 'rings', 'moment', 'impossible', 'tell', 'upper', 'hand', 'joe', 'get', 'hand', 'alpha', 'chin', 'strength', 'push', 'man', 'head', 'back', 'natural', 'angle', 'neck', 'alpha', 'go', 'still', 'joe', 'roll', 'pant', 'vision', 'return', 'blurry', 'room', 'come', 'slowly', 'back', 'focus', 'scrambles', 'feet', 'take', 'man', 'nina', 'joe', 'snatch', 'joe', 'pistol', 'hand', 'haul', 'ass', 'hallway', 'blood', 'spot', 'fall', 'floor', 'make', 'window', 'fire', 'escape', 'drop', 'bottom', 'level', 'fire', 'escape', 'gantry', 'joe', 'land', 'hard', 'manhattan', 'william', 'political', 'private', 'life', 'dog', 'court', 'settlement', 'reach', 'parent', 'girl', 'age', 'consent', 'joe', 'pay', 'cash', 'blood', 'hand', 'bill', 'pay', 'right', 'swell', 'thick', 'blood', 'matte', 'jaw', 'throat', 'cashier', 'raise', 'eyebrow', 'manhattan', 'joe', 'prop', 'side', 'building', 'rain', 'fall', 'around', 'sniff', 'area', 'joe', 'ease', 'open', 'mouth', 'dig', 'take', 'time', 'get', 'purchase', 'deep', 'inside', 'joe', 'suppresse', 'need', 'scream', 'close', 'broken', 'tooth', 'molar', 'split', 'two', 'joe', 'bloody', 'fingertip', 'set', 'joe', 'collapse', 'whisper', 'count', '52a', 'counting', 'overlap', '8', 'yr', 'old', 'boy', '8', 'joe', 'rear', 'door', 'refrigerate', 'truck', 'heavy', 'padlock', 'several', 'fbi', 'agent', 'stand', 'flashlight', 'fix', 'truck', 'close', 'dollar', 'bill', 'slowly', 'flap', 'puddle', 'heavy', 'duty', 'bolt', 'cropper', 'bite', 'harden', 'steel', 'rush', 'air', 'tall', 'rear', 'door', 'truck', 'swing', 'open', 'reveal', 'joe', 'rain', 'stream', 'brim', 'joe', 'pov', 'gloom', 'truck', 'interior', 'give', 'way', 'tableau', 'resemble', 'géricault', 'raft', 'medusa', 'four', 'chinese', 'girl', 'late', 'dead', 'barely', 'clothe', 'body', 'entwine', 'dark', 'eye', 'open', 'head', 'cast', 'mouth', 'agape', 'joe', 'closing', 'narrow', 'eye', 'two', 'dead', 'girl', 'old', 'face', 'frame', 'long', 'black', 'hair', 'cheek', 'press', 'together', 'eternal', 'embrace', 'close', 'still', 'distinct', 'mole', 'upper', 'lip', 'mouth', 'strain', 'wide', 'rise', 'pitch', 'dialling', 'tone', 'manhattan', 'lean', 'batter', 'pay', 'phone', 'fish', 'receiver', 'ear', 'cheek', 'wrap', 'pad', 'gauze', 'crudely', 'tape', 'place', 'blood', 'smear', 'face', 'hand', 'iodine', 'connect', 'straight', 'phone', 'company', 'digital', 'voicemail', '53a', 'mccleary', 'voice', 'mail', 'synthesized', 'robot', 'call', 'beat', 'follow', 'gruff', 'man', 'voice', 'mccleary', 'joe', 'immediately', 'hang', 'simple', 'white', 'clapboard', 'two', 'storey', 'affair', 'backing', 'ocean', 'view', 'pristine', 'year', 'old', 'caddy', 'park', 'driveway', 'classic', 'u', 'turn', 'diminish', 'turn', 'joe', 'watch', 'see', 'blink', 'signal', 'light', 'pass', 'gleam', 'house', 'brisk', 'move', 'joe', 'outline', 'pane', 'door', 'glass', 'pop', 'fall', 'shatter', 'close', 'free', 'hand', 'reach', 'feel', 'latch', 'joe', 'move', 'kitchen', 'radio', 'adjacent', 'living', 'area', 'huge', 'cat', 'lap', 'vast', 'pool', 'milk', 'upturned', 'look', 'meet', 'joe', 'gaze', 'return', 'milk', 'splatter', 'microwave', 'door', 'hang', 'open', 'old', 'school', 'coffee', 'percolator', 'stand', 'fridge', 'inch', 'stew', 'coffee', 'bubble', 'furiously', 'bottom', 'glass', 'jug', 'joe', 'pull', 'plug', 'wall', 'bed', 'unslept', 'flower', 'bedspread', 'tuck', 'neatly', 'side', 'make', 'tub', 'compact', 'perfume', 'bottle', 'atomizer', 'arrange', 'order', 'living', 'joe', 'sit', 'sofa', 'joe', 'evidence', 'place', 'sleep', 'blanket', 'empty', 'beer', 'bottle', 'old', 'fashioned', 'porno', 'mag', 'power', 'motoryacht', 'cupboard', 'young', 'slim', 'shoulder', 'shoulder', 'senator', 'snr', 'son', 'beside', 'stand', 'year', 'old', 'votto', 'hand', 'gold', 'clock', 'years', 'service', 'mood', 'smile', 'round', 'photograph', 'line', 'wall', 'mantelpiece', 'mark', 'progress', 'mccleary', 'state', 'trooper', 'portly', 'man', 'suit', 'interspersed', 'photograph', 'wife', 'different', 'age', 'image', 'love', 'couple', 'black', 'white', 'wedding', 'picture', 'handsome', 'pair', 'halloween', 'fancy', 'dress', 'costume', 'laurel', 'hardy', 'deck', 'beam', 'camera', 'tight', 'embrace', 'noticeably', 'gaunt', 'silk', 'scarf', 'tie', 'head', 'hiding', 'lack', 'hair', 'persian', 'cat', 'leap', 'coffee', 'table', 'back', 'arch', 'joe', 'run', 'hand', 'close', 'joe', 'palm', 'bright', 'red', 'streak', 'persian', 'cat', 'fur', 'tail', 'spatter', 'blood', 'fresh', 'joe', 'tense', 'house', 'joe', 'pull', 'open', 'bedside', 'drawer', 'royal', 'blue', 'colt', 'joe', 'spin', 'fully', 'load', 'living', 'car', 'key', 'familiar', 'cadillac', 'logo', 'key', 'ring', 'joe', 'grab', 'key', 'joe', 'look', 'death', 'drive', 'fast', 'limit', 'allow', 'sweat', 'dot', 'brow', 'constantly', 'check', 'rear', 'view', 'mirror', 'pass', 'car', 'road', 'wince', 'pop', 'one', 'painkiller', 'buy', 'midtown', 'drug', 'dealer', 'almost', 'impossible', 'part', 'lip', 'loll', 'slightly', 'forward', 'sound', 'car', 'engine', 'loud', 'quickening', 'continue', 'live', 'joe', 'yr', 'old', 'joe', 'bare', 'cheste', 'standing', 'poker', 'straight', 'face', 'wall', 'voice', 'adult', 'well', 'well', 'well', 'well', '8', 'yr', 'old', 'joe', 'begin', 'fall', 'asleep', 'head', 'bow', 'almost', 'graze', 'chintzy', 'wallpaper', 'voice', 'adult', 'touch', 'wall', '56a', 'joe', 'head', 'jolt', 'upright', 'slap', 'face', 'hard', 'speed', 'manic', 'sound', 'fly', 'buzz', 'trap', 'fluorescent', 'light', 'joe', 'prop', 'wall', 'elevator', 'door', 'begin', 'slide', 'shut', 'front', 'face', 'joe', 'let', 'room', 'room', 'wilt', 'flower', 'body', 'sit', 'high', 'back', 'desk', 'chair', 'wear', 'towel', 'dress', 'gown', 'slipper', 'eye', 'close', 'mouth', 'slightly', 'agape', 'shattered', 'rolodex', 'index', 'card', 'spread', 'glass', 'top', 'desk', 'blackjack', 'table', 'breath', 'john', 'card', 'bear', 'angel', 'detail', 'joe', 'turn', 'hand', 'dry', 'blood', 'fox', 'edge', 'dialling', 'tone', 'three', 'digit', 'punch', 'underneath', 'glass', 'tabletop', 'congeal', 'blood', 'pool', 'mccleary', 'hand', 'rest', 'top', 'surface', 'finger', 'bent', 'mangle', 'emergency', 'fire', 'department', 'see', 'smoke', 'come', 'old', 'lady', 'house', 'night', 'joe', 'terrify', 'face', 'moises', 'open', 'lip', 'synchronize', 'joe', 'call', 'sycamore', 'red', 'brick', 'house', 'knee', 'face', 'gun', 'press', 'angel', 'head', 'explosion', 'muzzle', 'flash', 'blood', 'spray', 'face', 'joe', 'mother', 'house', 'siren', 'haul', 'back', 'cab', 'fire', 'truck', 'caddy', 'park', 'opposite', 'joe', 'sit', 'forward', 'scan', 'view', 'windscreen', 'fire', 'truck', 'pull', 'away', 'reveal', 'black', 'dodge', 'challenger', 'conspicuous', 'genteel', 'street', 'joe', 'look', 'low', 'roof', 'house', 'joe', 'bedroom', 'day', 'window', 'see', 'joe', 'haul', 'low', 'roof', 'continue', 'climb', '2nd', 'floor', 'house', 'mother', 'joe', 'drop', 'view', 'exterior', 'balcony', 'turn', 'window', 'face', 'stare', 'joe', 'pov', 'mother', 'bed', 'tear', 'powder', 'burn', 'scorch', 'pillow', 'case', 'yr', 'old', 'joe', 'shin', 'angry', 'red', 'welt', 'stockinge', 'foot', 'step', 'specific', 'floorboard', 'joe', 'bedroom', 'mother', 'house', '8', 'yr', 'old', 'joe', 'crouch', 'wardrobe', 'hide', 'clean', 'shallow', 'breathing', 'distort', 'face', 'internally', 'beat', 'creaking', 'shoe', 'downstairs', 'floorboard', 'muffle', 'sound', 'man', 'raise', 'voice', 'yr', 'old', 'joe', 'hold', 'breath', 'listen', 'terrify', 'slaps', 'thuds', 'woman', 'moan', 'sobbing', 'yr', 'old', 'joe', 'breathe', 'relief', 'face', 'red', 'shame', 'internally', 'beat', '8', 'yr', 'old', 'joe', 'clamp', 'hand', 'ear', 'house', 'joe', 'stand', 'top', 'stair', 'unobserved', 'see', 'shadow', 'move', 'living', 'room', 'dining', 'area', 'downstairs', 'music', 'kitchen', 'radio', 'drift', 'joe', 'lift', 'scorch', 'pillow', 'mother', 'face', 'reach', 'close', 'joe', 'gently', 'set', 'glass', 'bedside', 'table', 'hear', 'aide', 'leave', 'shatter', 'dripping', 'blood', 'house', 'day', 'area', 'floor', 'live', 'room', 'floor', 'joe', 'slump', 'beside', 'foot', 'stride', 'away', 'bottle', 'whiskey', 'swing', 'free', 'hand', '60a', 'ball', 'peen', 'drop', 'floor', '8', 'yr', 'old', 'joe', 'watch', 'sofa', 'mother', 'stare', 'blood', 'run', 'top', 'stair', 'eye', 'closed', 'concentration', 'finger', 'move', 'trigger', 'gun', 'come', 'stair', 'express', 'train', 'gun', 'draw', 'one', 'continuous', 'move', 'take', 'spin', 'fire', 'two', 'shot', 'quick', 'succession', 'long', 'eerie', 'silence', 'break', 'beep', 'incoming', 'text', 'message', 'faint', 'sound', 'gunman', 'fingernail', 'scratch', 'floor', 'reveal', 'one', 'gunman', 'keel', 'dead', 'belly', 'crawl', 'away', 'kitchen', 'stay', 'low', 'joe', 'stand', 'body', 'dead', 'gunman', 'rifle', 'jacket', 'pocket', 'phone', 'read', 'text', 'joe', 'type', 'quiet', 'hit', 'send', 'close', 'joe', 'hand', 'fill', 'glass', 'water', 'pop', 'hit', 'hear', 'kuwait', 'play', 'radio', 'grab', 'chintzy', 'tea', 'towel', 'slide', 'sink', 'unit', 'floor', 'bottle', 'pill', 'midtown', 'drug', 'dealer', 'joe', 'shake', 'pill', 'downs', 'one', 'lie', 'slump', 'joe', 'bad', 'shape', 'bleed', 'bullet', 'stomach', 'joe', 'toss', 'chintzy', 'tea', 'towel', 'use', 'free', 'hand', 'limply', 'clutch', 'stomach', 'staunch', 'bleed', '61a', 'joe', 'crawl', 'feeds', 'pill', 'hold', 'glass', 'water', 'lip', 'joe', 'pull', 'sit', 'position', 'gunman', 'look', 'unfocused', 'girl', 'far', 'away', 'eye', 'glaze', 'safe', 'dirty', 'old', 'man', 'joe', 'side', 'gunman', 'rest', 'finger', 'man', 'neck', 'feel', 'pulse', 'work', 'way', 'round', 'gunman', 'smirk', 'look', 'pass', 'wish', 'talk', 'want', 'talk', 'run', 'place', 'friendly', 'face', 'paradise', 'never', 'know', '62a', 'gunman', 'suddenly', 'wide', 'awake', 'stare', 'hyper', 'alertness', 'reach', 'joe', 'hand', 'hold', 'tight', 'girl', 'gunman', 'stare', 'past', 'joe', 'gently', 'take', 'gunman', 'hand', 'still', 'clutch', 'blood', 'soak', 'chintzy', 'tea', 'towel', 'stomach', 'old', 'man', 'know', 'press', 'firmly', 'gunman', 'nose', 'mouth', 'joe', 'bedroom', 'mother', 'house', 'later', 'duct', 'tape', 'wrench', 'roll', 'mirror', 'joe', 'redress', 'jaw', 'duct', 'tape', 'dress', 'seal', 'neck', 'low', 'ear', 'hand', 'lift', 'mother', 'foot', 'gently', 'remove', 'tiny', 'floral', 'slipper', 'place', 'floor', 'foot', 'bed', 'close', 'joe', 'face', 'lift', 'mother', 'delicately', 'shoulder', 'frail', 'back', 'come', 'view', 'long', 'silver', 'hair', 'fall', 'loose', 'chignon', 'joe', 'blink', 'almost', 'crack', 'second', 'lay', 'mother', 'gently', 'back', 'bed', 'joe', 'comb', 'tangle', 'hair', 'smooth', 'brush', 'dresser', 'hairbrush', 'set', 'table', 'wisp', 'silver', 'hair', 'waft', 'joe', 'hold', 'nostril', 'inhale', 'close', 'eye', 'later', 'joe', 'close', 'joe', 'finger', 'toggle', 'window', 'switch', 'garbage', 'bag', 'ripple', 'noisily', 'breeze', 'joe', 'sunken', 'eye', 'rim', 'cold', 'sweat', 'glue', 'rear', 'view', 'mirror', 'reflection', 'black', 'plastic', 'cling', 'mother', 'face', 'define', 'outline', 'feature', 'death', 'mask', 'joe', 'send', 'window', 'back', 'later', 'take', 'harlem', 'river', 'north', 'low', 'level', 'george', 'washington', 'cross', 'jersey', 'later', 'joe', 'drive', 'river', 'dam', 'new', 'jersey', 'later', 'wind', 'woodland', 'steep', 'dirt', 'path', 'cradling', 'mother', 'body', 'footfall', 'falter', 'uneven', 'ground', '64a', 'arch', 'branch', 'open', 'reveal', 'edge', 'vast', 'lake', 'goldenrod', 'pages', 'new', 'jersey', 'shroud', 'form', 'joe', 'mother', 'lie', 'state', 'broad', 'rock', 'water', 'edge', 'close', 'joe', 'hand', 'precisely', 'place', 'neat', 'row', 'personal', 'effect', 'photograph', 'gun', 'foliage', 'edge', 'lake', 'hand', 'scoop', 'heavy', 'flat', 'rock', 'foot', 'stumble', 'coarse', 'shale', 'lake', 'bed', 'joe', 'wade', 'chest', 'deep', 'river', 'push', 'corpse', 'ahead', 'flat', 'rock', 'weigh', 'barely', 'touch', 'surface', 'water', 'gently', 'immerse', 'plastic', 'bundle', 'fingertip', 'tear', 'open', 'small', 'hole', 'side', 'head', 'white', 'hair', 'pour', 'black', 'water', 'wafting', 'reed', 'silently', 'black', 'plastic', 'crimp', 'leach', 'joe', 'walk', 'deep', 'small', 'body', 'slowly', 'sink', 'surface', 'arm', 'water', 'reach', 'eye', 'underwater', 'pocket', 'suit', 'lakeside', 'take', 'step', 'forward', 'disappear', 'surface', 'underwater', 'limitless', 'black', 'void', 'joe', 'mother', 'embrace', 'black', 'shroud', 'cocoon', 'tight', 'around', 'seem', 'tiny', 'size', 'child', 'cling', 'silver', 'hair', 'loosen', 'waft', 'water', 'whirl', 'head', 'shoulder', 'glint', 'beckon', 'strand', 'light', 'joe', 'mother', 'shroud', 'descend', 'depth', 'joe', 'drift', 'away', '65a', 'crack', 'muffle', 'surface', 'water', 'almighty', 'subaqueous', 'crash', 'thirty', 'foot', 'mass', 'dead', 'tree', 'smash', 'surface', 'almost', 'top', 'joe', 'branch', 'spear', 'underwater', 'joe', 'eye', 'snap', 'open', 'underwater', 'churn', 'bubble', 'trunk', 'wide', 'eye', 'close', 'mouth', 'chest', 'clench', 'joe', 'pov', 'underwater', 'limitless', 'black', 'void', 'diaphanous', 'figure', 'nina', 'nightdress', 'flow', 'follow', 'clothe', 'dripping', 'rhythm', 'shoe', 'sound', 'countdown', 'continuous', 'joe', 'head', 'lean', 'glass', 'window', 'leech', 'moisture', 'seat', 'countdown', 'continuous', 'voice', 'merge', 'beat', 'train', 'track', 'joe', 'stare', 'unblinke', 'business', 'compartment', 'window', 'nearby', 'man', 'reach', 'suitcase', 'overhead', 'walk', 'aisle', 'joe', 'take', 'moment', 'notice', 'black', 'shiny', 'shoe', 'whisper', 'man', 'shiny', 'black', 'shoe', 'slip', 'feet', 'follow', 'feet', 'ledge', 'roof', 'tell', 'mommy', 'montage', 'image', 'dialogue', 'overlapping', 'confuse', 'underscored', 'rhythm', 'train', 'track', 'joe', 'imagination', 'williams', 'cry', 'nina', 'back', 'turn', 'lotte', 'joe', 'imagination', 'williams', 'cry', 'votto', 'take', 'lotte', 'joe', 'imagination', 'joe', 'hand', 'firm', 'handshake', 'place', 'hand', 'top', 'joe', 'intimate', 'pain', 'whisper', 'clean', 'mess', 'joe', 'imagination', 'smiley', 'face', 'car', 'window', 'nina', 'begin', 'run', 'tear', 'clown', 'lotte', 'joe', 'imagination', 'raise', 'forefing', 'lip', 'ina', 'gesture', 'secrecy', 'tiny', 'foot', 'disappear', 'view', 'door', 'room', 'snap', 'shut', 'dart', 'black', 'tree', 'trunk', 'stuttering', 'view', 'brief', 'glimpse', 'go', 'joe', 'mother', 'joe', 'face', 'clear', 'plastic', 'bag', 'suddenly', 'suck', 'skin', 'distort', 'feature', 'inhuman', 'mask', 'name', 'nina', 'mention', 'constituency', 'office', 'day', 'sign', 'life', 'solitary', 'intern', 'mans', 'phone', 'street', 'joe', 'watch', 'campaign', 'office', 'front', 'seat', 'rental', 'car', 'wear', 'dishevel', 'sunday', 'lift', 'bar', 'mouth', 'take', 'bite', 'chocolate', 'closer', 'joe', 'exit', 'constituency', 'meeting', 'accompany', 'two', 'surly', 'dark', 'suit', 'lead', 'joe', 'attention', 'drift', 'votto', 'one', 'open', 'rear', 'door', 'votto', 'car', 'conceal', 'gun', 'holster', 'peep', 'jacket', 'opening', '2nd', 'grasp', 'votto', 'elbow', 'edge', 'seat', 'man', 'wear', 'earpiece', 'joe', 'watch', 'car', 'slowly', 'pull', 'away', 'face', 'rear', 'set', 'grim', 'expression', 'joe', 'hand', 'ball', 'hershey', 'wrapper', 'drop', 'passenger', 'seat', 'land', 'identically', 'screw', 'wrapper', 'room', 'bathroom', 'night', 'steam', 'rise', 'bathtub', 'illuminate', 'room', 'shimmer', 'steam', 'joe', 'lie', 'water', 'wet', 'face', 'towel', 'obscure', 'eye', 'mouth', 'close', 'breathe', 'eye', 'socket', 'sink', 'hang', 'bathroom', 'door', 'wrap', 'albany', 'dry', 'plastic', 'bag', 'duct', 'tape', 'cable', 'tie', 'lie', 'motel', 'bed', 'sound', 'tear', 'duct', 'pause', 'tear', 'sound', 'wet', 'laboured', 'breathing', 'joe', 'country', 'joe', 'hand', 'rest', 'wheel', 'hire', 'car', 'distant', 'shape', 'car', 'trace', 'country', 'road', 'joe', 'drive', 'eye', 'implacable', 'fix', 'road', 'ahead', 'rabbit', 'flare', 'glare', 'car', 'headlamp', 'run', 'ahead', 'mesmerically', 'sync', 'speed', 'car', 'nina', 'foot', 'run', 'fast', 'road', 'ahead', 'williams', 'build', 'colonial', 'style', 'joe', 'step', 'view', 'hammer', 'hand', 'williams', 'joe', 'run', 'camera', 'hammer', 'raise', 'williams', 'angle', 'mansion', 'dark', 'one', 'light', 'window', 'top', 'floor', 'williams', 'house', 'gate', 'bethlehem', 'angle', 'swimming', 'pool', 'williams', 'house', 'gate', 'bethlehem', 'dusk', 'lie', 'face', 'incapacitate', 'williams', 'house', 'night', 'joe', 'move', 'brightly', 'light', 'kitchen', 'leave', 'guard', 'wake', 'joe', 'slowly', 'move', 'joe', 'gently', 'climb', 'staircase', 'hammer', 'jut', 'back', 'pocket', 'pant', 'williams', 'plush', 'carpet', 'move', 'upper', 'landing', 'leave', 'open', 'door', 'regal', 'master', 'bedroom', 'empty', 'king', 'bed', 'undisturbed', 'joe', 'move', 'long', 'wood', 'panel', 'corridor', 'draw', 'mysterious', 'sound', 'muffle', 'music', '72a', 'whisper', 'overlap', 'shhhh', 'goldenrod', 'pages', 'joe', 'imagination', 'williams', 'fix', 'toy', 'shelf', 'tiny', 'furniture', 'bear', 'smell', 'nina', 'pillow', 'touches', 'material', 'clothe', 'cheek', 'smoothes', 'bed', 'joe', 'approach', 'foot', 'bed', 'stop', 'stare', 'hold', 'joe', 'face', 'look', 'suddenly', 'change', 'well', 'sir', 'well', 'sir', 'well', 'sir', 'well', 'repeat', 'repetition', 'amazing', 'high', 'energy', 'pop', 'song', 'swell', 'full', 'pitch', 'stop', 'repeat', 'listen', 'music', 'joe', 'euphoric', 'elated', 'light', 'way', 'house', 'discover', 'williams', 'corpse', 'joe', 'see', 'joe', 'mother', 'joe', 'four', 'chinese', 'girl', 'joe', 'descend', 'stair', 'trance', 'see', 'hand', 'old', 'lady', 'joe', 'mother', 'room', 'williams', 'house', '172a', 'joe', 'see', 'young', 'boy', 'stand', '8', 'year', 'old', 'self', 'well', 'sir', 'well', 'sir', 'well', 'sir', 'well', 'see', 'man', 'towel', 'head', 'man', 'russian', 'bath', 'joe', 'late', 'dining', 'williams', 'house', 'continuous', '172b', 'joe', 'stand', 'sentry', 'gas', 'pump', 'dollar', 'slowly', 'climb', 'digital', 'display', 'interminable', 'joe', 'sit', 'table', 'fix', 'previously', 'stand', 'eileen', 'barton', 'come', 'have', 'drift', 'tinny', 'reflect', 'plastic', 'surface', 'traveller', 'populate', 'diner', 'middle', 'aged', 'sit', 'oxygen', 'tank', 'draw', 'breath', 'hand', 'hold', 'mask', 'woman', 'teenage', 'schoolwork', 'spread', 'front', 'elderly', 'couple', 'loudly', 'discuss', 'imminent', 'hospital', 'visit', 'pair', 'suits', 'commuter', 'route', 'city', 'joe', 'suck', 'last', 'drop', 'milk', 'shake', 'metal', 'cup', 'joe', 'drink', 'quickly', 'brain', 'freeze', 'painful', 'work', 'reach', 'second', 'milkshake', 'group', 'three', 'empty', 'milkshake', 'glass', 'cacophony', 'voice', 'rise', 'music', 'punctuate', 'wheeze', 'oxygen', 'inhalation', 'get', 'take', 'pill', 'frank', 'equal', 'take', 'anymore', 'drives', 'dream', 'stare', 'straight', 'ahead', 'joe', 'reach', 'overcoat', 'pocket', 'pull', 'mccleary', 'snub', 'nose', 'muzzle', 'push', 'firmly', 'fires', 'explosion', 'blood', 'exit', 'wind', 'spray', 'direction', 'splatter', 'table', 'chair', 'donut', 'diner', 'commuter', 'couple', 'breakfast', 'plate', 'keep', 'eat', 'man', 'oxygen', 'keep', 'gulp', 'woman', 'daughter', 'homework', 'ink', 'red', 'blood', 'drip', 'wobble', 'chin', 'fork', 'full', 'runny', 'egg', 'mouth', 'pour', 'huge', 'dollop', 'syrup', 'bloody', 'pancake', '78a', 'easy', 'syrup', 'frank', 'know', 'allow', 'tha', 'goldenrod', 'pages', 'mouthful', 'goddammit', 'blood', 'pump', 'edge', 'joe', 'table', 'flow', 'thickly', 'tile', 'floor', 'foot', 'nimbly', 'step', 'spread', 'pool', 'brim', 'coffee', 'decant', 'hand', 'joe', 'joe', 'alive', 'turn', 'measuredly', 'face', 'look', 'joe', 'lift', 'coffee', 'decant', 'indicate', 'offer', 'refill', 'joe', 'look', 'ground', 'beside', 'pool', 'blood', 'nina', 'bare', 'foot', 'joe', 'joe', 'look', 'hold', 'cup', 'save', 'life', 'waitres', 'smile', 'fill', 'joe', 'cup', 'know', 'need', 'least', 'five', 'cup', 'morning', 'get', 'engine', 'run', 'beat', 'ok', 'honey', 'beat', 'look', 'death', 'joe', 'smile', 'wake', 'rote', 'well', 'nice', 'day', 'sincerely', 'nice', 'day', 'look', 'window', 'waitress', 'catch', 'eye', 'line', 'take', 'view', 'turn', 'head', 'waitress', 'coo', 'mewl', 'newborn', 'mother', 'arm', 'low', 'building', 'travel', 'orange', 'dawn', 'catch', 'fire', 'edge', 'shroud', 'nice', 'day', 'curt', 'bill', 'tuck', 'edge', 'joe', 'coffee', 'cup', 'bloody', 'thumb', 'print', 'glisten', 'treasurer', 'signature', 'waitress', 'take', 'bill', 'table', 'blankly', 'stare', 'joe', 'go', 'joe', 'stride', 'parking', 'lot', 'face', 'focusse', 'movement', 'purposeful', 'black', 'trunk', 'rental', 'swing', 'open', 'reveal', 'joe', 'toss', 'glock', 'joe', 'pov', 'dead', 'pistol', 'suppressor', 'revolver', 'three', 'new', 'hammer', 'tool', 'trade', 'triplicate', 'rental', 'car', 'fire', 'car', 'ignition', 'radio', 'come', 'close', 'joe', 'sit', 'wheel', 'joe', 'car', 'pull', 'diner', 'lot', 'gee', 'joe', 'rental', 'car', 'interstate', 'first', 'ray', 'rise', 'sun', 'flare', 'lens', 'sear', 'white', 'light', 'illuminate', 'freeway', 'sign', 'joe', 'eye', 'fix', 'road', 'ahead', 'face', 'set', 'look', 'sheer', 'determination']\n"
     ]
    }
   ],
   "source": [
    "print(type(sents))\n",
    "print(sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6862\n"
     ]
    }
   ],
   "source": [
    "print(len(sents[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995\n"
     ]
    }
   ],
   "source": [
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow = Word2Vec(\n",
    "    sentences=trigrams[sents],\n",
    "    vector_size=64,\n",
    "    window=8,\n",
    "    sg=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors = cbow.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "cbow.save(r\"C:\\Users\\bened\\DataScience\\ANLP\\AT2\\models\\final_cbow.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "del cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vectors.save(r\"C:\\Users\\bened\\DataScience\\ANLP\\AT2\\models\\final_wv.wordvectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['belted', 'morlocks', '11_-', 'yell', 'candlelit', 'snapshots', 'startlingly', 'hyde_park', 'eyeshade', 'machine_guns']\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "vocab = list(word_vectors.key_to_index.keys())\n",
    "print(random.sample(vocab, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995\n"
     ]
    }
   ],
   "source": [
    "print(len(sents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now need to attach the original keys to sents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to fuck:\n",
      "fucking: 0.9588\n",
      "asshole: 0.9057\n",
      "shit: 0.8898\n",
      "bitch: 0.8775\n",
      "fuckin: 0.8614\n"
     ]
    }
   ],
   "source": [
    "def find_synonyms(vectors, word, topn=5):\n",
    "    synonyms = vectors.most_similar(word, topn=topn)\n",
    "    print(f\"Words most similar to {word}:\")\n",
    "    for w, s in synonyms:\n",
    "        print(f'{w}: {s:.4f}')\n",
    "\n",
    "find_synonyms(word_vectors, 'fuck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to sex:\n",
      "masturbate: 0.7412\n",
      "lesbian: 0.7181\n",
      "girlfriend: 0.6957\n",
      "fantasize: 0.6926\n",
      "sexual: 0.6868\n"
     ]
    }
   ],
   "source": [
    "find_synonyms(word_vectors, 'sex', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to cocaine:\n",
      "heroin: 0.8568\n",
      "coke: 0.7380\n",
      "drug: 0.7149\n",
      "packet: 0.7128\n",
      "vitamin: 0.7116\n"
     ]
    }
   ],
   "source": [
    "find_synonyms(word_vectors, 'cocaine', 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words most similar to naked:\n",
      "nude: 0.8235\n",
      "wrap_towel: 0.6653\n",
      "topless: 0.6474\n",
      "underwear: 0.6417\n",
      "shirtless: 0.6343\n"
     ]
    }
   ],
   "source": [
    "find_synonyms(word_vectors, 'naked', 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature and Target Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode Target (Aus ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Argentina:13, Australia:M, Brazil:14, Canada:P...\n",
       "1    Argentina:13, Australia:M, Canada:PG::(Alberta...\n",
       "2    Argentina:13, Australia:PG, Brazil:10, Canada:...\n",
       "3    Australia:MA, Finland:K-15, France:Tous public...\n",
       "4    Argentina:16, Argentina:18::(cable rating), Au...\n",
       "Name: age restrict, dtype: object"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta_df['age restrict'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age restrict\n",
      "M        994\n",
      "MA15+    392\n",
      "PG       348\n",
      "MA       195\n",
      "R        128\n",
      "G        108\n",
      "A         39\n",
      "R18+      36\n",
      "SOA       18\n",
      "NRC       10\n",
      "RC         1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_aus_rating(string):\n",
    "    pattern = re.compile(r'(?:^|, )Australia:([A-Z]{1,3}\\d{0,2}\\+?)[,]|$')\n",
    "    match = re.search(pattern, string)\n",
    "    if match:\n",
    "        return match.group(1)\n",
    "    else:\n",
    "        return pd.NA\n",
    "\n",
    "# drop rows with NA for age restrict \n",
    "meta_df = meta_df.dropna(how='any', subset='age restrict')\n",
    "\n",
    "aus_classifications = meta_df['age restrict'].apply(extract_aus_rating)\n",
    "print(aus_classifications.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From Wiki: \n",
    "\n",
    "Early film classification\n",
    "The Commonwealth Film Censorship Board was created in 1917 to view, classify, and censor films imported from overseas. In the early years of the system there were 3 ratings:[4]\n",
    "\n",
    "G for \"general audiences\"\n",
    "A for \"not suitable for children\"\n",
    "SOA for \"suitable for adults only\"\n",
    "All ratings were advisory in nature and while distributors were required to display them on advertising, there were no restrictions on children's attendance. As such, films with adult ratings were still routinely censored.\n",
    "\n",
    "A almost certainly corresponds to G. \n",
    "NRC is effectively PG. \n",
    "SOA is more ambiguous, we'll drop this. \n",
    "Extreme imbalance for 'RC': only one sample means it can't be represented in both training and testing data, so can't be used. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "age restrict\n",
       "2.0    994\n",
       "3.0    587\n",
       "1.0    358\n",
       "4.0    164\n",
       "0.0    147\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aus_target_map = {\n",
    "    'G': 0,\n",
    "    'A': 0,\n",
    "    'PG': 1,\n",
    "    'NRC': 1,\n",
    "    'M': 2,\n",
    "    'MA': 3,\n",
    "    'MA15+': 3,\n",
    "    'R': 4,\n",
    "    'R18+': 4\n",
    "}\n",
    "\n",
    "y = aus_classifications.map(aus_target_map)\n",
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2250,)\n"
     ]
    }
   ],
   "source": [
    "y = y.dropna()\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_class_balance(y):\n",
    "    classes = sorted(list(y.unique()))\n",
    "    value_counts = y.value_counts()\n",
    "    for c in classes:\n",
    "        class_ratio = value_counts[c] / len(y) * 100\n",
    "        print(f\"{c} represents {class_ratio}% of target data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 represents 6.533333333333332% of target data\n",
      "1 represents 15.911111111111111% of target data\n",
      "2 represents 44.17777777777778% of target data\n",
      "3 represents 26.08888888888889% of target data\n",
      "4 represents 7.28888888888889% of target data\n"
     ]
    }
   ],
   "source": [
    "compute_class_balance(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a significant class imbalance with regard to movies rated M (3); the other imbalances are less significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n"
     ]
    }
   ],
   "source": [
    "keys = list(data.keys())\n",
    "print(keys[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['@1', 'roxbury', '\\n ', 'write', 'steve', 'koren', 'ferrell', 'chris', 'kattan', 'june', '\\n ', '@1', 'hear', 'love', 'haddaway', 'night', 'fall', 'partytime', 'begin', 'superimpose']\n"
     ]
    }
   ],
   "source": [
    "print(data[0][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1995\n"
     ]
    }
   ],
   "source": [
    "print(len(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: \n",
    "- convert '@n' to n \n",
    "- mark sequence boundaries at '\\n' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [@1, roxbury, \\n , write, steve, koren, ferrel...\n",
       "1    [@0, first, gold, light, dapple, valley, seem,...\n",
       "2    [1blue, clouds, 2ext, @1, flat, horizon, stret...\n",
       "3    [lee, \\n , voice, malcolm, \\n , hoodwink, take...\n",
       "4    [@0, float, steep, scrubby, slope, hear, male,...\n",
       "dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# X = 'Year' + data matched on index labels \n",
    "year = meta_df['year']\n",
    "data_series = pd.Series(data)\n",
    "data_series.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Index Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1633, 3)\n"
     ]
    }
   ],
   "source": [
    "# concatenate the three series together to ensure same indices \n",
    "df = pd.concat([year, data_series, y], axis=1, join='inner')\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "del year, data_series, data, meta_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1633 observations is perfectly fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['year','screenplay','target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['year', 'screenplay']]\n",
    "y = df.pop('target')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(r\"C:\\Users\\bened\\DataScience\\ANLP\\AT2\\36118_NLP_Spring\\data_splits\\X_train_2.csv\")\n",
    "X_test.to_csv(r\"C:\\Users\\bened\\DataScience\\ANLP\\AT2\\36118_NLP_Spring\\data_splits\\X_test_2.csv\")\n",
    "\n",
    "y_train.to_csv(r\"C:\\Users\\bened\\DataScience\\ANLP\\AT2\\36118_NLP_Spring\\data_splits\\y_train_2.csv\")\n",
    "y_test.to_csv(r\"C:\\Users\\bened\\DataScience\\ANLP\\AT2\\36118_NLP_Spring\\data_splits\\y_test_2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Build Vocab "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vocab(data):\n",
    "    vocab = set()\n",
    "    for tokens in data:\n",
    "        vocab.update(tokens)\n",
    "    return {word: idx + 1 for idx, word in enumerate(vocab)}\n",
    "\n",
    "train_vocab = create_vocab(X_train['screenplay'])\n",
    "test_vocab = create_vocab(X_test['screenplay'])\n",
    "vocab = {**train_vocab, **test_vocab}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>screenplay</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>792</th>\n",
       "      <td>2009</td>\n",
       "      <td>[idiot, fit, screenplay, fiem, screenplay, a-1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>2011</td>\n",
       "      <td>[screenplay, james, l., brooks, \\n , @0, brisk...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1842</th>\n",
       "      <td>1996</td>\n",
       "      <td>[@0, boss, jonathan, goldstein, john, francis,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1999</td>\n",
       "      <td>[@0, hannah, allen, \\n , @1, last, credit, app...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>2011</td>\n",
       "      <td>[@1, @0, open, city, series, brief, shot, defi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      year                                         screenplay\n",
       "792   2009  [idiot, fit, screenplay, fiem, screenplay, a-1...\n",
       "403   2011  [screenplay, james, l., brooks, \\n , @0, brisk...\n",
       "1842  1996  [@0, boss, jonathan, goldstein, john, francis,...\n",
       "8     1999  [@0, hannah, allen, \\n , @1, last, credit, app...\n",
       "332   2011  [@1, @0, open, city, series, brief, shot, defi..."
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.series.Series'>\n"
     ]
    }
   ],
   "source": [
    "screenplays = X_train['screenplay'] + X_test['screenplay']\n",
    "print(type(screenplays))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mscreenplays\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'float' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "print(screenplays[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'float'>\n"
     ]
    }
   ],
   "source": [
    "print(type(screenplays[792]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "\n",
    "def preprocess_for_phrases(data):\n",
    "\n",
    "    labeled_sentences = {}\n",
    "\n",
    "    for screenplay_idx, tokens in data.items():\n",
    "\n",
    "        current_sentence = []\n",
    "        processed_line = []\n",
    "        line_label = None\n",
    "\n",
    "        for token in tokens:\n",
    "            \n",
    "            # parse label\n",
    "            if token.startswith('@'):\n",
    "                line_label = np.int8(token[1:])\n",
    "                labeled_sentences[line_label] = []\n",
    "\n",
    "            # skip if token contains non-word chars \n",
    "            elif re.search(r'\\W', token) and token not in {'.', '\\n'}:\n",
    "                continue\n",
    "\n",
    "            elif token == ':':\n",
    "                continue\n",
    "\n",
    "            # skip over '-' \n",
    "            elif token == '-':\n",
    "                continue\n",
    "            \n",
    "            elif token == '.':\n",
    "                if current_sentence:\n",
    "                    processed_line.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            \n",
    "            elif token == '\\n':\n",
    "                if current_sentence:\n",
    "                    processed_line.append(current_sentence)\n",
    "                    current_sentence = []\n",
    "            \n",
    "            else:\n",
    "                current_sentence.append(token)\n",
    "        \n",
    "        if current_sentence:\n",
    "            processed_line.append(current_sentence)\n",
    "        \n",
    "        if line_label is not None:\n",
    "            labeled_sentences[line_label].extend(processed_line)\n",
    "        \n",
    "    return labeled_sentences\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Token:Integer Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "792     [idiot, fit, screenplay, fiem, screenplay, a-1...\n",
       "403     [screenplay, james, l., brooks, \\n , @0, brisk...\n",
       "1842    [@0, boss, jonathan, goldstein, john, francis,...\n",
       "8       [@0, hannah, allen, \\n , @1, last, credit, app...\n",
       "332     [@1, @0, open, city, series, brief, shot, defi...\n",
       "                              ...                        \n",
       "1782    [write, saulnier, \\n , @0, draft-, white, \\n ,...\n",
       "388     [@1, house, corpse, revise, living, see, littl...\n",
       "1702    [shawn, christensen, \\n , @0, card, december, ...\n",
       "1576    [write, silent, film, illustrate, musically, t...\n",
       "1980    [@1, woman, drive, country, road, field, side,...\n",
       "Name: screenplay, Length: 1306, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train['screenplay']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_seqs(data, vocab):\n",
    "    return [[vocab[token] for token in tokens] for tokens in data]\n",
    "\n",
    "X_train_seq = tokens_to_seqs(X_train['screenplay'], vocab)\n",
    "X_test_seq = tokens_to_seqs(X_test['screenplay'], vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1306,) + inhomogeneous part.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\bened\\anaconda3\\envs\\spacy_env\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2022\u001b[0m, in \u001b[0;36mshape\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m   2021\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2022\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\n\u001b[0;32m   2023\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train_seq\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(X_test_seq))\n",
      "File \u001b[1;32mc:\\Users\\bened\\anaconda3\\envs\\spacy_env\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:2024\u001b[0m, in \u001b[0;36mshape\u001b[1;34m(a)\u001b[0m\n\u001b[0;32m   2022\u001b[0m     result \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   2023\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:\n\u001b[1;32m-> 2024\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43masarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m   2025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence. The requested array has an inhomogeneous shape after 1 dimensions. The detected shape was (1306,) + inhomogeneous part."
     ]
    }
   ],
   "source": [
    "print(np.shape(X_train_seq))\n",
    "print(np.shape(X_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'list'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train_seq))\n",
    "print(type(X_test_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[51870, 6650, 62248, 101830, 62248, 119097, 59314, 27781, 128261, 69104]\n"
     ]
    }
   ],
   "source": [
    "print(X_train_seq[0][:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train avg. sequence length: 10227.97549770291\n",
      "Test avg sequence length: 10007.085626911316\n"
     ]
    }
   ],
   "source": [
    "# average sequence length \n",
    "import numpy as np\n",
    "\n",
    "avg_seq_length_train = np.mean([len(seq) for seq in X_train_seq])\n",
    "avg_seq_length_test = np.mean([len(seq) for seq in X_test_seq])\n",
    "\n",
    "print(\"Train avg. sequence length:\", avg_seq_length_train)\n",
    "print(\"Test avg sequence length:\", avg_seq_length_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34025\n",
      "21527\n"
     ]
    }
   ],
   "source": [
    "print(np.max([len(seq) for seq in X_train_seq]))\n",
    "print(np.max([len(seq) for seq in X_test_seq]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence lengths fall below 15527.905686500992 with 95% confidence\n"
     ]
    }
   ],
   "source": [
    "X_combined = X_train_seq + X_test_seq\n",
    "seq_lengths = np.array([len(seq) for seq in X_combined])\n",
    "se = np.mean(seq_lengths) + 2 * np.std(seq_lengths)\n",
    "print(f\"Sequence lengths fall below {se} with 95% confidence\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq=15000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "truncate to 10k at start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "X_train_padded = pad_sequences(X_train_seq, maxlen=max_seq, padding='post', truncating='pre')\n",
    "X_test_padded = pad_sequences(X_test_seq, maxlen=max_seq, padding='post', truncating='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1306, 15000)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X_train_seq, X_test_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_years = np.array(X_train['year'].values).reshape(-1,1)\n",
    "test_years = np.array(X_test['year'].values).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dict = dict()\n",
    "\n",
    "glove_path = r\"C:\\\\Users\\bened\\DataScience\\ANLP\\AT2\\\\glove.6B.100d.txt\"\n",
    "\n",
    "glove_file = open(glove_path, encoding='utf-8')\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dims = asarray(records[1:], dtype='float32')\n",
    "    embeddings_dict[word] = vector_dims\n",
    "\n",
    "glove_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(188894, 100)\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = zeros((len(vocab), 100))\n",
    "for word, index in vocab.items():\n",
    "    vector = embeddings_dict.get(word)\n",
    "    if vector is not None:\n",
    "        embedding_matrix[index] = vector\n",
    "\n",
    "print(embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "del embeddings_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'> int32\n",
      "<class 'pandas.core.series.Series'> int64\n",
      "<class 'pandas.core.series.Series'> int32\n"
     ]
    }
   ],
   "source": [
    "print(type(X_train_padded), X_train_padded.dtype)\n",
    "print(type(X_train['year']), X_train['year'].dtype)\n",
    "print(type(y_train), y_train.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'values'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m X_train_year \u001b[38;5;241m=\u001b[39m X_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[0;32m      2\u001b[0m X_test_year \u001b[38;5;241m=\u001b[39m X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m----> 3\u001b[0m y_train \u001b[38;5;241m=\u001b[39m \u001b[43my_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'values'"
     ]
    }
   ],
   "source": [
    "X_train_year = X_train['year'].values\n",
    "y_train = y_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_year = X_test['year'].values.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(X_train['year'].isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reattach_sents(data_dict, sents):\n",
    "    return {key: sent_list for key, sent_list in zip(data_dict.keys(), sents)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "sents_dict = reattach_sents(data, sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data, sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "del aus_classifications, bigrams_select"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vector_transform(sentences, phrase_model, word_vectors):\n",
    "    vectorized_sents = []\n",
    "    for sent in sentences:\n",
    "        trigram_sent = trigrams[sent]\n",
    "        vectorized_sent = []\n",
    "        for word in trigram_sent:\n",
    "            if word in word_vectors:\n",
    "                vectorized_sent.append(word_vectors[word])\n",
    "            else:\n",
    "                vectorized_sent.append(np.zeros(word_vectors.vector_size))\n",
    "        vectorized_sents.append(vectorized_sent)\n",
    "    return vectorized_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = {\n",
    "        key: vector_transform(sentences, frozen_trigrams, word_vectors)\n",
    "        for key, sentences in sents_dict.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "def pad_sentences(vectorized_sents, maxlen, vector_size):\n",
    "    padded_sents = pad_sequences(\n",
    "        vectorized_sents,\n",
    "        maxlen=maxlen,\n",
    "        dtype='float16',\n",
    "        padding='post',\n",
    "        truncating='pre',\n",
    "        value=np.zeros(vector_size)\n",
    "    )\n",
    "    return padded_sents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_data = {\n",
    "    key: pad_sentences(vectorized_data[key], 15000, 64)\n",
    "        for key in vectorized_data\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_vectors(data, sents, phrase_model, word_vectors, maxlen):\n",
    "    indexed_sents = reattach_sents(data, sents)\n",
    "    vector_size = word_vectors.vector_size\n",
    "    vectorized_data = {\n",
    "        key: vector_transform(sentences, phrase_model, word_vectors)\n",
    "        for key, sentences in indexed_sents.items()\n",
    "    }\n",
    "    padded_data = {\n",
    "        key: pad_sentences(vectorized_data[key], maxlen, vector_size)\n",
    "    for key in vectorized_data\n",
    "    }\n",
    "    return padded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "del bigrams_dict, data_series, df, keys, meta_df, sents_data, trigrams_df, trigrams_dict, vocab, X, X_test, X_train, y, y_test, y_train, year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 31.8 GiB for an array with shape (8880, 15000, 64) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[76], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m maxlen\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15000\u001b[39m\n\u001b[1;32m----> 2\u001b[0m padded_vectors \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_vectors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrigrams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_vectors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[70], line 8\u001b[0m, in \u001b[0;36mprepare_vectors\u001b[1;34m(data, sents, phrase_model, word_vectors, maxlen)\u001b[0m\n\u001b[0;32m      3\u001b[0m vector_size \u001b[38;5;241m=\u001b[39m word_vectors\u001b[38;5;241m.\u001b[39mvector_size\n\u001b[0;32m      4\u001b[0m vectorized_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     key: vector_transform(sentences, phrase_model, word_vectors)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, sentences \u001b[38;5;129;01min\u001b[39;00m indexed_sents\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m      7\u001b[0m }\n\u001b[1;32m----> 8\u001b[0m padded_data \u001b[38;5;241m=\u001b[39m \u001b[43m{\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpad_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorized_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvectorized_data\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m\u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m padded_data\n",
      "Cell \u001b[1;32mIn[70], line 9\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      3\u001b[0m vector_size \u001b[38;5;241m=\u001b[39m word_vectors\u001b[38;5;241m.\u001b[39mvector_size\n\u001b[0;32m      4\u001b[0m vectorized_data \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      5\u001b[0m     key: vector_transform(sentences, phrase_model, word_vectors)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, sentences \u001b[38;5;129;01min\u001b[39;00m indexed_sents\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m      7\u001b[0m }\n\u001b[0;32m      8\u001b[0m padded_data \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m----> 9\u001b[0m     key: \u001b[43mpad_sentences\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvectorized_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m vectorized_data\n\u001b[0;32m     11\u001b[0m }\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m padded_data\n",
      "Cell \u001b[1;32mIn[72], line 4\u001b[0m, in \u001b[0;36mpad_sentences\u001b[1;34m(vectorized_sents, maxlen, vector_size)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpad_sentences\u001b[39m(vectorized_sents, maxlen, vector_size):\n\u001b[1;32m----> 4\u001b[0m     padded_sents \u001b[38;5;241m=\u001b[39m \u001b[43mpad_sequences\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvectorized_sents\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaxlen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncating\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpre\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalue\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m padded_sents\n",
      "File \u001b[1;32mc:\\Users\\bened\\anaconda3\\envs\\spacy_env\\Lib\\site-packages\\keras\\src\\utils\\sequence_utils.py:113\u001b[0m, in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(value, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dtype_str:\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    108\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`dtype` \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not compatible with `value`\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms type: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mYou should set `dtype=object` for variable length \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    110\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrings.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m     )\n\u001b[1;32m--> 113\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfull\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmaxlen\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msample_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(sequences):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(s):\n",
      "File \u001b[1;32mc:\\Users\\bened\\anaconda3\\envs\\spacy_env\\Lib\\site-packages\\numpy\\core\\numeric.py:329\u001b[0m, in \u001b[0;36mfull\u001b[1;34m(shape, fill_value, dtype, order, like)\u001b[0m\n\u001b[0;32m    327\u001b[0m     fill_value \u001b[38;5;241m=\u001b[39m asarray(fill_value)\n\u001b[0;32m    328\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m fill_value\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m--> 329\u001b[0m a \u001b[38;5;241m=\u001b[39m empty(shape, dtype, order)\n\u001b[0;32m    330\u001b[0m multiarray\u001b[38;5;241m.\u001b[39mcopyto(a, fill_value, casting\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munsafe\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 31.8 GiB for an array with shape (8880, 15000, 64) and data type float32"
     ]
    }
   ],
   "source": [
    "maxlen=15000\n",
    "padded_vectors = prepare_vectors(data, sents, trigrams, word_vectors, maxlen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FF NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bened\\anaconda3\\envs\\spacy_env\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ screenplay_input    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15000</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ classifiable_eleme… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │     <span style=\"color: #00af00; text-decoration-color: #00af00\">90,006</span> │ screenplay_input… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ classifiable_ele… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ classifiable_ele… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ classifiable_ele… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ classifiable_ele… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ classifiable_ele… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Lambda</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ classifiable_ele… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Impact_score_0      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ lambda[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Impact_score_1      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ lambda_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Impact_score_2      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ lambda_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Impact_score_3      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ lambda_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Impact_score_4      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ lambda_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Impact_score_5      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> │ lambda_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ Impact_score_0[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ Impact_score_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ Impact_score_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ Impact_score_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ Impact_score_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│                     │                   │            │ Impact_score_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ year_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ year_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_layer        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ screenplay_input    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15000\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ classifiable_eleme… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │     \u001b[38;5;34m90,006\u001b[0m │ screenplay_input… │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda (\u001b[38;5;33mLambda\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ classifiable_ele… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_1 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ classifiable_ele… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_2 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ classifiable_ele… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_3 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ classifiable_ele… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_4 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ classifiable_ele… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lambda_5 (\u001b[38;5;33mLambda\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ classifiable_ele… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Impact_score_0      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2\u001b[0m │ lambda[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Impact_score_1      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2\u001b[0m │ lambda_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Impact_score_2      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2\u001b[0m │ lambda_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Impact_score_3      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2\u001b[0m │ lambda_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Impact_score_4      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2\u001b[0m │ lambda_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ Impact_score_5      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m2\u001b[0m │ lambda_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ Impact_score_0[\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ Impact_score_1[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ Impact_score_2[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ Impact_score_3[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ Impact_score_4[\u001b[38;5;34m0\u001b[0m… │\n",
       "│                     │                   │            │ Impact_score_5[\u001b[38;5;34m0\u001b[0m… │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ year_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m… │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ year_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ output_layer        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │         \u001b[38;5;34m40\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│ (\u001b[38;5;33mDense\u001b[0m)             │                   │            │                   │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,058</span> (351.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m90,058\u001b[0m (351.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">90,058</span> (351.79 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m90,058\u001b[0m (351.79 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "maxlen=15000\n",
    "\n",
    "# input for text data \n",
    "text_input = layers.Input(shape=(maxlen,), name='screenplay_input')\n",
    "\n",
    "# densely connected layer that tries to boil down feature space to six classifiable elements\n",
    "dense = layers.Dense(6, activation='relu', name='classifiable_elements')(text_input)\n",
    "\n",
    "def one_to_one(inputs):\n",
    "    return layers.multiply(inputs)\n",
    "\n",
    "impact_units = [layers.Dense(1, activation='relu', name=f\"Impact_score_{i}\")(layers.Lambda(lambda x: x[:,i:i+1])(dense)) for i in range(6)]\n",
    "\n",
    "# concatenate 1:1 impact scores\n",
    "impact_scores = layers.concatenate(impact_units)\n",
    "\n",
    "# year \n",
    "year_input = layers.Input(shape=(1,),name='year_input')\n",
    "\n",
    "# concatenate impact scores with year input\n",
    "merged = layers.concatenate([impact_scores, year_input])\n",
    "\n",
    "# output layer \n",
    "output = layers.Dense(5, activation='softmax', name='output_layer')(merged)\n",
    "\n",
    "ff = Model(inputs=[text_input, year_input], outputs=output)\n",
    "\n",
    "ff.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['f1_score'])\n",
    "\n",
    "ff.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_padded' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(\u001b[43mX_train_padded\u001b[49m))\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(X_train_year))\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(np\u001b[38;5;241m.\u001b[39mshape(y_train))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_padded' is not defined"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.shape(X_train_padded))\n",
    "print(np.shape(X_train_year))\n",
    "print(np.shape(y_train))\n",
    "print(np.shape(X_test_padded))\n",
    "print(np.shape(X_test_year))\n",
    "print(np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bened\\anaconda3\\envs\\spacy_env\\Lib\\site-packages\\keras\\src\\models\\functional.py:225: UserWarning: The structure of `inputs` doesn't match the expected structure: ['screenplay_input', 'year_input']. Received: the structure of inputs=('*', '*')\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 62ms/step - accuracy: 0.0735 - loss: 751.9108 - val_accuracy: 0.0826 - val_loss: 609.5391\n",
      "Epoch 2/8\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - accuracy: 0.0752 - loss: 616.3588 - val_accuracy: 0.0826 - val_loss: 587.6800\n",
      "Epoch 3/8\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0814 - loss: 591.7157 - val_accuracy: 0.0826 - val_loss: 565.8665\n",
      "Epoch 4/8\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0882 - loss: 574.0881 - val_accuracy: 0.0826 - val_loss: 544.0271\n",
      "Epoch 5/8\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.1240 - loss: 540.1219 - val_accuracy: 0.1590 - val_loss: 532.4036\n",
      "Epoch 6/8\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.1515 - loss: 534.4742 - val_accuracy: 0.0826 - val_loss: 514.9621\n",
      "Epoch 7/8\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step - accuracy: 0.0798 - loss: 521.8160 - val_accuracy: 0.0826 - val_loss: 501.1057\n",
      "Epoch 8/8\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - accuracy: 0.0835 - loss: 493.8526 - val_accuracy: 0.1590 - val_loss: 485.8084\n"
     ]
    }
   ],
   "source": [
    "ff_history = ff.fit(\n",
    "    [X_train_padded, X_train_year], \n",
    "    y_train,\n",
    "    validation_data=([X_test_padded, X_test_year], y_test),\n",
    "    epochs=8, \n",
    "    batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n"
     ]
    }
   ],
   "source": [
    "print(type(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type int).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[62], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mff\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss:\u001b[39m\u001b[38;5;124m\"\u001b[39m, score[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Accuracy:\u001b[39m\u001b[38;5;124m\"\u001b[39m, score[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[1;32mc:\\Users\\bened\\anaconda3\\envs\\spacy_env\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\bened\\anaconda3\\envs\\spacy_env\\Lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type int)."
     ]
    }
   ],
   "source": [
    "score = ff.evaluate(X_test, y_test, verbose=1)\n",
    "\n",
    "print(\"Test Loss:\", score[0])\n",
    "print(\"Test Accuracy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM, Embedding, Dense, Bidirectional, Dropout, Input, concatenate\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen=10000\n",
    "text_input = Input(shape=(maxlen,),name='screenplay_input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\bened\\anaconda3\\envs\\spacy_env\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ screenplay_input    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)     │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>,     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">18,889,400</span> │ screenplay_input… │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)        │     <span style=\"color: #00af00; text-decoration-color: #00af00\">42,240</span> │ embedding[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">390</span> │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">42</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ year_input          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)         │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],    │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │                   │            │ year_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">40</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ screenplay_input    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)     │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ embedding           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m,     │ \u001b[38;5;34m18,889,400\u001b[0m │ screenplay_input… │\n",
       "│ (\u001b[38;5;33mEmbedding\u001b[0m)         │ \u001b[38;5;34m100\u001b[0m)              │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)        │     \u001b[38;5;34m42,240\u001b[0m │ embedding[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │        \u001b[38;5;34m390\u001b[0m │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m6\u001b[0m)         │         \u001b[38;5;34m42\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]       │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ year_input          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)         │          \u001b[38;5;34m0\u001b[0m │ dense_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],    │\n",
       "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │                   │            │ year_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)         │         \u001b[38;5;34m40\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,932,112</span> (72.22 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,932,112\u001b[0m (72.22 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">42,712</span> (166.84 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m42,712\u001b[0m (166.84 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,889,400</span> (72.06 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m18,889,400\u001b[0m (72.06 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "embedding_layer = Embedding(len(vocab), 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)(text_input)\n",
    "\n",
    "lstm_layer = LSTM(64, return_sequences=False)(embedding_layer)\n",
    "\n",
    "dense_1 = Dense(6, activation='relu')(lstm_layer)\n",
    "dense_2 = Dense(6, activation='relu')(dense_1)\n",
    "\n",
    "year_input = Input(shape=(1,),name='year_input')\n",
    "\n",
    "merged = concatenate([dense_2, year_input])\n",
    "\n",
    "output = Dense(5, activation='softmax')(merged)\n",
    "\n",
    "model = Model(inputs=[text_input, year_input],outputs=output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, test_vocab, train_vocab, vocab, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del aus_classifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 3s/step - accuracy: 0.3720 - loss: 2.0058 - val_accuracy: 0.4343 - val_loss: 1.8320\n",
      "Epoch 2/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.3913 - loss: 2.0180 - val_accuracy: 0.4404 - val_loss: 1.8539\n",
      "Epoch 3/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.3905 - loss: 1.9294 - val_accuracy: 0.3670 - val_loss: 1.8346\n",
      "Epoch 4/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 3s/step - accuracy: 0.3717 - loss: 1.9945 - val_accuracy: 0.3394 - val_loss: 1.8804\n",
      "Epoch 5/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3s/step - accuracy: 0.3643 - loss: 1.9885 - val_accuracy: 0.3914 - val_loss: 1.8230\n",
      "Epoch 6/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.3887 - loss: 1.9378 - val_accuracy: 0.3609 - val_loss: 1.8359\n",
      "Epoch 7/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.3784 - loss: 1.9681 - val_accuracy: 0.3058 - val_loss: 1.8863\n",
      "Epoch 8/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.3403 - loss: 2.1082 - val_accuracy: 0.3731 - val_loss: 1.8790\n",
      "Epoch 9/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.3930 - loss: 2.0202 - val_accuracy: 0.3578 - val_loss: 1.8715\n",
      "Epoch 10/10\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m67s\u001b[0m 3s/step - accuracy: 0.3720 - loss: 2.0325 - val_accuracy: 0.4190 - val_loss: 1.8537\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f9808439d0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [X_train_padded, X_train['year']], \n",
    "    y_train,\n",
    "    validation_data=([X_test_padded, X_test['year']], y_test),\n",
    "    epochs=10, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "increasing batch size seems to be effective, but epochs not so much. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without second dense layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(len(vocab), 100, weights=[embedding_matrix], input_length=maxlen, trainable=False)(text_input)\n",
    "\n",
    "lstm_layer = LSTM(64, return_sequences=False)(embedding_layer)\n",
    "\n",
    "dense_1 = Dense(6, activation='relu')(lstm_layer)\n",
    "dense_2 = Dense(6, activation='relu')(dense_1)\n",
    "\n",
    "year_input = Input(shape=(1,),name='year_input')\n",
    "\n",
    "merged = concatenate([dense_2, year_input])\n",
    "\n",
    "output = Dense(5, activation='softmax')(merged)\n",
    "\n",
    "model = Model(inputs=[text_input, year_input],outputs=output)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
